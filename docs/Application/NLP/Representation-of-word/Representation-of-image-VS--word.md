# 前言

对word的representation要比对image的representation难，因为word是具有非常广泛的distribution，它的组合是非常之多的，但是image则非常集中；

在NLP领域一个非常重要的问题就是如何来度量语义的相似性；

## representation of word

[Vector space model](https://en.wikipedia.org/wiki/Vector_space_model) 

[Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)

在[Word embedding](https://en.wikipedia.org/wiki/Word_embedding)中有这样的一段话：

>   In 2000 [Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) et al. provided in a series of papers the "Neural probabilistic language models" to reduce the high dimensionality of words representations in contexts by "learning a distributed representation for words". (Bengio et al., 2003).[[12\]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-12)  	

