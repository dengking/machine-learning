# AI

## Regularization 

https://en.wikipedia.org/wiki/Regularization_(mathematics)

https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/

## dropout

https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/

https://stackoverflow.com/questions/35545798/keep-prob-in-tensorflow-mnist-tutorial





# TODO

## [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)



## [Graphical model](https://en.wikipedia.org/wiki/Graphical_model)



[Structured prediction](https://en.wikipedia.org/wiki/Structured_prediction)

- [Graphical models](https://en.wikipedia.org/wiki/Graphical_model)
  - [Bayes net](https://en.wikipedia.org/wiki/Bayesian_network)
  - [Conditional random field](https://en.wikipedia.org/wiki/Conditional_random_field)
  - [Hidden Markov](https://en.wikipedia.org/wiki/Hidden_Markov_model)



## [Helmholtz machine](https://en.wikipedia.org/wiki/Helmholtz_machine)

[restricted Boltzmann machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)





http://cs230.stanford.edu/blog/createtrainmodel/

https://danijar.com/patterns-for-fast-prototyping-with-tensorflow/


https://hackernoon.com/towards-ai-how-long-does-it-take-you-to-go-from-idea-to-working-prototype-a-day-a-month-8a03ffecca0a

# tf.contrib.layers.xavier_initializer
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/xavier_initializer?hl=cy

https://stackoverflow.com/questions/42286426/what-is-the-difference-between-xavier-initializer-and-xavier-initializer-conv2d?rq=1

# position embedding

https://stackoverflow.com/questions/44614603/what-is-the-the-position-embedding-in-the-convolutional-sequence-to-sequence-lea


# time serial forecast
https://www.tensorflow.org/tutorials/structured_data/time_series

# attention

## self attention 

[Self-Attention Mechanisms in Natural Language Processing](https://dzone.com/articles/self-attention-mechanisms-in-natural-language-proc)

## transformer
https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/



# attention and model architecture

目前所阅读的如下论文中，使用attention的model architecture都是encoder-decoder

- 201409 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
- 201502 [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf?)

当然除此之外，还有论文一种architecture：transformer，那transformer中是如何的呢？还需要阅读它的paper；


# drop-out


# 构造图的过程和Futures and promises之间的关系
https://en.wikipedia.org/wiki/Futures_and_promises



# tensorflow的测试

在搭建后来computation graph后，是可以使用numpy数据进行测试的，如：

 https://stackoverflow.com/questions/51443898/how-do-i-use-tf-reshape 



# how to find tensorflow bottleneck


# 构建一个完整的AI应用

我目前的一个问题是还没有独立地完成一个AI任务，需要找时间来练习。


# use knowledge graph  to enhance semantic comprehension

https://www.aminer.cn/research_report/5dd163d61ca12517163f032c?download=false
> 来到2019年的今天，深度学习的诸多局限性也慢慢得到广泛认知。对于自然语言处理而言，要做到精细深度的语义理解，单纯依靠数据标注与算力投入无法解决本质问题。如果没有先验知识的支持，“中国的乒乓球谁都打不过”与“中国的足球谁都打不过”，在计算机看来语义上并没有巨大差异，而实际上两句中的“打不过”意思正好相反。因此，融入知识来进行知识指导的自然语言处理，是通向精细而深度的语言理解的必由之路。然而，这些知识又从哪里来呢？这就涉及到人工智能的一个关键研究问题——知识获取。

# knowledge graph

https://www.zhihu.com/topic/19551341/hot

https://www.zhihu.com/org/hua-wei-yun-ji-zhu-zhai-ji-di/activities

https://www.zhihu.com/people/ji-yi-chao/activities

https://www.zhihu.com/question/354059866/answer/907009362

https://www.zhihu.com/topic/19838204/hot