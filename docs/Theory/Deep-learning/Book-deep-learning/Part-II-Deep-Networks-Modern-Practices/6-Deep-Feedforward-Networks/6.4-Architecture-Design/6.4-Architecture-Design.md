# 6.4 Architecture Design

Another key design consideration for neural networks is determining the architecture. The word **architecture** refers to the overall structure of the network: how many units it should have and how these units should be connected to each other.

Most neural networks are organized into groups of units called **layers**. Most neural network architectures arrange these **layers** in a **chain structure**, with each **layer** being a function of the layer that preceded it. In this structure, the first layer is given by
$$
h^{(1)}:=g^{(1)}(W^{(1)T}x) + b^{(1)})
$$
the second layer is given by
$$
h^{(2)}:=g^{(2)}(W^{(2)T} h^{(1)}) + b^{(2)})
$$
and so on.

In these chain-based architectures, the main architectural considerations are to choose the depth of the network and the width of each layer.

## 6.4.1 Universal Approximation Properties and Depth

> NOTE: 参见 [Universal-approximation-theorem](./Universal-approximation-theorem.md)

## 6.4.2 Other Architectural Considerations	

So far we have described neural networks as being simple chains of layers, with the main considerations being the depth of the network and the width of each layer. In practice, neural networks show considerably more diversity.

Many neural network architectures have been developed for specific tasks. Specialized architectures for computer vision called convolutional networks are described in chapter 9. Feedforward networks may also be generalized to the recurrent neural networks for sequence processing, described in chapter 10, which 
have their own architectural considerations.

In general, the layers need not be connected in a chain, even though this is the most common practice.  Many architectures build a main chain but then add extra architectural features to it, such as skip connections going from layer $i$ to layer $i +2$ or higher. These skip connections make it easier for the gradient to flow from output layers to layers nearer the input.

Another key consideration of architecture design is exactly how to connect a pair of layers to each other. In the default neural network layer described by a linear transformation via a matrix $W$ , **every** input unit is connected to every output unit. Many specialized networks in the chapters ahead have fewer connections, so that each unit in the **input layer** is connected to only a small subset of units in the **output layer**. These strategies for reducing the number of connections reduce the number of **parameters** and the amount of computation required to evaluate the network, but are often highly problem-dependent. For example, **convolutional networks**, described in chapter 9 , use specialized patterns of sparse connections that are very effective for computer vision problems. In this chapter, it is difficult to give much more specific advice concerning the architecture of a **generic** neural network. Subsequent chapters develop the particular architectural strategies that have been found to work well for different application domains.

