# 6.5 Back-Propagation and Other Differentiation Algorithms

When we use a feedforward neural network to accept an input $x$ and produce an output  $\hat{y}$, information flows forward through the network. The inputs $x$ provide the initial information that then propagates up to the hidden units at each layer and finally produces $\hat{y}$ . This is called **forward propagation** . During training,
**forward propagation** can continue onward until it produces a scalar cost $J( \theta )$. The back-propagation algorithm ( Rumelhart et al. 1986a ), often simply called  **backprop**, allows the information from the cost to then flow backwards through the network, in order to compute the gradient.

Computing an analytical expression for the gradient is straightforward, but numerically evaluating such an expression can be computationally expensive. The back-propagation algorithm does so using a simple and inexpensive procedure.

The term back-propagation is often misunderstood as meaning the whole learning algorithm for multi-layer neural networks. Actually, back-propagation refers only to the method for computing the **gradient**, while another algorithm, such as **stochastic gradient descent**, is used to perform learning using this **gradient**. Furthermore, back-propagation is often misunderstood as being specific to multi-layer neural networks, but in principle it can compute **derivatives** of any function (for some functions, the correct response is to report that the derivative of the function is undefined). Specifically, we will describe how to compute the **gradient** ∇ x f ( x y , ) for an arbitrary function f , where x is a set of variables whose derivatives are desired, and $y$ is an additional set of variables that are inputs to the function but whose derivatives are not required. In learning algorithms, the **gradient** we most often require is the gradient of the **cost function** with respect to the **parameters**, ∇ θ J ( θ ). Many machine learning tasks involve computing other derivatives, either as part of the learning process, or to analyze the learned model. The **back-propagation algorithm** can be applied to these tasks as well, and is not restricted to computing the **gradient** of the **cost function** with respect to the **parameters**. The idea of computing derivatives by propagating information through a network is very general, and can be used to compute values such as the **Jacobian** of a function f with multiple outputs. We restrict our description here to the most commonly used case where has a single output.

## 6.5.1 Computational Graphs

So far we have discussed neural networks with a relatively informal graph language. To describe the back-propagation algorithm more precisely, it is helpful to have a more precise language.

Many ways of formalizing computation as graphs are possible.

Here, we use each node in the graph to indicate a variable. The variable may be a scalar, vector, matrix, tensor, or even a variable of another type. To formalize our graphs, we also need to introduce the idea of an operation . An operation is a simple function of one or more variables. Our graph language is accompanied by a set of allowable operations. Functions more complicated than the operations in this set may be described by composing many operations together.

Without loss of generality, we define an operation to return only a single output variable. This does not lose generality because the output variable can have multiple entries, such as a vector. Software implementations of back-propagation usually support operations with multiple outputs, but we avoid this case in our description because it introduces many extra details that are not important to conceptual understanding. 

If a variable y is computed by applying an operation to a variable x , then we draw a directed edge from x to y . We sometimes annotate the output node with the name of the operation applied, and other times omit this label when the operation is clear from context.

Examples of computational graphs are shown in figure 6.8. 

![](./Figure-6.8.jpg)





> NOTE: 
>
> ### 思考: 如何根据函数表达式来构造computational graph
>
> 这个过程跟compiler parse我们编写的program是类似的，函数表达式和我们的program都是遵循grammar的，compiler根据grammar（往往使用production的方式来表达）使用grammar tree来表示我们的program；我们也可以根据function expression的grammar构造出它的computational graph，其实上述computational graph非常类似于abstract syntax tree的；在`Programming\software-TensorFlow\Implementation\TensorFlow-white-paper\Computation-graph-VS-parse-tree.md`中也对这个问题进行了分析。
>
> computational graph的构造构成是可以参考parsing的过程的。

## 6.5.2 Chain Rule of Calculus

The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions whose derivatives are known. Back-propagation is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient.

Let x be a real number, and let f and g both be functions mapping from a real number to a real number. Suppose that y = g ( x ) and z = f ( g ( x )) = f ( y ). Then the chain rule states that



We can generalize this beyond the scalar case. Suppose that x ∈ R m , y ∈ R n ,g maps from R m to R n , and f maps from R n to R . If y = g ( x ) and z = f ( y ), then



(6.45)

In vector notation, this may be equivalently written as

(6.46)

where ∂y/∂x is the Jacobian matrix of g.

> NOTE: 式(6.45)和式(6.46)，可以参见维基百科[Chain rule#General rule](https://en.wikipedia.org/wiki/Chain_rule#General_rule)

> NOTE: 式6.46是一个递归定义

From this we see that the gradient of a variable x can be obtained by multiplying a Jacobian matrix ∂y/∂x by a gradient ∇ y z . The back-propagation algorithm consists of performing such a Jacobian-gradient product for each operation in the graph.



Usually we do not apply the back-propagation algorithm merely to vectors, but rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the same as back-propagation with vectors. The only difference is how the numbers are arranged in a grid to form a tensor. We could imagine flattening each tensor into a vector before we run back-propagation, computing a vector-valued gradient, and then reshaping the gradient back into a tensor. In this rearranged view, back-propagation is still just multiplying Jacobians by gradients. 



To denote the gradient of a value z with respect to a tensor X , we write ∇ X z , just as if X were a vector. The indices into X now have multiple coordinates—for example, a 3-D tensor is indexed by three coordinates. We can abstract this away by using a single variable i to represent the complete tuple of indices. For all
possible index tuples i , ( ∇ X z ) i gives ∂z ∂X i . This is exactly the same as how for all possible integer indices i into a vector, ( ∇ x z ) i gives ∂z ∂x i . Using this notation, we can write the chain rule as it applies to tensors. If and , then



(6.47)

> NOTE: 式(6.47)和式(6.45)和式(6.46)，本质相同，只是表示形式不同而已

> NOTE: 上述式子仅仅给出了非常简单的复合函数，实际的full-connected network远比它要复杂。所以需要使用更加高级、更加general的描述方式：[Vector calculus](https://en.wikipedia.org/wiki/Vector_calculus)，所有的都要以[tensor](https://en.wikipedia.org/wiki/Tensor)为单位，与此相关的概念有：
>
> - [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)
>
> 



## 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop

Using the chain rule, it is straightforward to write down an algebraic expression for the gradient of a **scalar** with respect to any node in the computational graph that produced that scalar. However, actually evaluating that expression in a computer introduces some extra considerations.

> NOTE: 对于neural network，上面这段话中的scalar所指为cost

Specifically, many subexpressions may be repeated several times within the overall expression for the gradient. Any procedure that computes the gradient will need to choose whether to store these subexpressions or to recompute them several times. An example of how these repeated subexpressions arise is given in figure 6.9.  In some cases, computing the same subexpression twice would simply be wasteful. For complicated graphs, there can be exponentially many of these wasted computations, making a naive implementation of the **chain rule** infeasible. In other cases, computing the same subexpression twice could be a valid way to reduce memory consumption at the cost of higher runtime.

> NOTE: tradeoff

We first begin by a version of the **back-propagation algorithm** that specifies the actual **gradient computation** directly (algorithm 6.2 along with algorithm 6.1 for the associated forward computation), in the order it will actually be done and according to the recursive application of chain rule. One could either directly perform these computations or view the description of the algorithm as a **symbolic specification**
of the **computational graph** for computing the back-propagation. However, this formulation does not make explicit the manipulation and the construction of the **symbolic graph** that performs the **gradient computation**. Such a formulation is presented below in section 6.5.6, with algorithm 6.5, where we also generalize to nodes that contain arbitrary tensors.

> NOTE: 

> NOTE: 理解上面这段话可以从这样的问题出发：给定一个function，我们可以画出它的computational graph，那如何来计算它的梯度呢？显然，对于computer而言的，我们需要总结出algorithm: algorithm 6.2 是计算gradient的algorithm；算法6.1是forward computation。
>
> 上面这段话的后面两段是对algorithm 6.2 的描述：
>
> - 我们可以将algorithm 6.2看做是: a **symbolic specification** of the **computational graph** for computing the back-propagation
> - algorithm 6.2并没有告诉我们:  the manipulation and the construction of the **symbolic graph** that performs the **gradient computation**. 
>
> “symbolic graph”是什么？我认为它是计算gradient的computational graph。

First consider a computational graph describing how to compute a single scalar $u^{(n)}$ (say the **loss** on a training example). This scalar is the quantity whose gradient we want to obtain, with respect to the $n_i$ input nodes $u^{(1)}$ to $u^{(n_i )}$ . In other words we wish to compute $ \frac { \partial u^{(n)} } { \partial u^{(i)} }$ for all $i \in \{ 1 , 2 ,...,n_i \}$ . In the application of back-propagation to computing gradients for gradient descent over parameters, $u^{ ( n ) }$ will be the cost associated with an example or a minibatch, while $u^{(1)}$ to $u^{(n_i )}$ correspond to the parameters of the model.

We will assume that the nodes of the graph have been ordered in such a way that we can compute their output one after the other, starting at $u^{(n_i +1)}$ and going up to $u^{n}$. As defined in algorithm 6.1, each node $u ^{( i )}$  is associated with an operation $f^{( i )}$  and is computed by evaluating the function
$$
u^{(i)} = f(\mathbb A^{(i)})   \tag {6.48}
$$
where $\mathbb A^{( i )}$  is the set of all nodes that are parents of $u^{(i)}$ .

> NOTE: 原文作者变量命名的方式是非常不易懂的。搞懂这些变量命名的前提是了解作者的命名规则：作者是以graph的node为单位来进行命名的，即$n_i$是对node的编号， $u^{(i)}$对应的是node的output。
>
> 一个graph是有input nodes的：$n_i$ input nodes $u^{(1)}$ to $u^{(n_i )}$ ；显然input nodes的值是可以直接获取的；其他的node的output是通过式$(6.48)$计算得到的。
>
> 式(6.48)所描述的是前馈过程，即 forward propagation computation，它所描述的是一个node的计算。

![](./algorithm-6.1.jpg)

> NOTE: 上述算法中，$Pa$是$Parent$的缩写；$Pa(u^{(i)})$表示节点$u^{(i)}$的父节点。

That algorithm specifies the **forward propagation computation**, which we could put in a graph $\mathcal{G}$ . In order to perform **back-propagation**, we can construct a **computational graph** that depends on $\mathcal{G}$ and adds to it an extra set of nodes. These form a subgraph $\mathcal{B}$ with one node per node of $\mathcal{G}$ . Computation in $\mathcal{B}$ proceeds in exactly the reverse of the order of computation in $\mathcal{G}$ , and each node of $\mathcal{B}$ computes the derivative $\frac { \partial u^{(n)} } { \partial u^{(i)} }$associated with the forward graph node $u^{(i)}$ . This is done using the **chain rule** with respect to scalar output $u ^ { (n) }$ :

![](./algorithm-6.2.jpg)

> NOTE: $grad\_table[u^{(n)}] \leftarrow 1$ 表示将 node $u^{(n)}$的gradient初始化为1；
>
> 要计算$grad\_table[u^{(j)}]$的值，需要知道$u^{(j)}$的所有的子节点，即上述算法中的$u^{(i)}$，$grad\_table[u^{(i)}]$的值已经计算得到，因为是方向是back（从后往前），$\frac {u^{(i)}} {u^{(j)}}$其实可以看做是$u^{(j)}$到$u^{(i)}$的边，有了这些认知，就可以上述`for`循环的循环体了。
>
> 上述使用$i:j \in Pa(u^{(i)})$来表示$u^{(j)}$的所有的子节点，它的表面意思是不容易理解的，它的表面意思是：$j$在$Pa(u^{(i)})$中，引申一下就是$j$是$u^{(i)}$的父节点；

Back-propagation thus avoids the exponential explosion in repeated subexpressions. However, other algorithms may be able to avoid more subexpressions by performing simplifications on the computational graph, or may be able to conserve memory by recomputing rather than storing some subexpressions. We will revisit these ideas after describing the back-propagation algorithm itself.

> NOTE: 如何来构造computational graph来使common subexpression称为common node in the graph？原书并没有给出algorithm。

## 6.5.4 Back-Propagation Computation in Fully-Connected MLP

To clarify the above definition of the back-propagation computation, let us consider the specific graph associated with a fully-connected multi-layer MLP.

Algorithm 6.3 first shows the forward propagation, which maps parameters to the supervised loss $L( \hat y, y)$,  associated with a single (input,target) training example $(x, y) $ ,  with $\hat y$ the output of the neural network when is provided in input.

Algorithm  then shows the corresponding computation to be done for applying the back-propagation algorithm to this graph.

Algorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and straightforward to understand. However, they are specialized to one specific problem.

Modern software implementations are based on the generalized form of back-propagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic computation.

![](./algorithm-6.3.jpg)



![](./algorithm-6.4.jpg)

> NOTE: 
>
> 上述直接从algorithm 6.3过渡到algorithm 6.4，一时半会可能无法理解。
>
> 维基百科Backpropagation：
>
> > Note the distinction: during model evaluation, the weights are fixed, while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training, the input–output pair is fixed, while the weights vary, and the network ends with the loss function.
>
> algorithm 6.3对应的是model evaluation；algorithm 6.4对应的是model training。
>
> ### Gradient descent
>
> algorithm 6.4给出的是计算gradient的algorithm，要充分理解它，我们需要首先从目的出发来思考：为什么要计算gradient？目的是使用gradient descend algorithm来使cost function达到极值，从而“学习”到使模型拟合效果最佳的**parameter**；对于cost function而言，它的自变量是parameter，如果用我们在高中数学中的问题来类比的话，可以使用二次函数$y = a x^2 + b x + c$，给定$a, b, c$，也就确定了一个二次函数，令它的倒数为0，我们就可以计算得到它的极值点，从而得到$x$的值；但是对于neural model而言，它的cost function是非凸的，所以training的过程是循序渐进（使用gradient descend逐步更新parameter直到cost function收敛），而非一蹴而就（对于凸函数，直接令导数为0，从而找到极值点）；
>
> 当我们决定采用一个neural model，我们会确定它的hyper parameter，会initialize它的parameter（一般是一个random值），会指定它的activation function，简而言之，会完全的确定一个function（model function），进而完全地确定对应的cost function；
>
> 我们首先执行forward propagation（evaluation的过程），我们采用algorithm 6.3，进而计算得到cost function的值；
>
> 然后执行back-propagation（training的过程），我们首先要计算得到cost function在某点的gradient（经过上面的分析我们已经知道了，此时的cost function是固定的，通过对于给定example，我们是可以计算得到它的对应的cost function的一个点的），由于cost function已知，所以我们就可以使用**链式法则**计算得到gradient的值，然后用计算得到的gradient来更新cost function的自变量即model function的parameter的值，显然我们这样做的目的是期望cost function走向一个极值点。
>
> 按照上述过程循环往复，知道cost function收敛。
>
> 需要注意的是：在training过程中，整个样本数据是会划分为各个batch的，然后逐个batch、逐个batch地feed到model中，每个batch的训练中，采用的不是同一个cost function（但是是同一类型的），在每个batch中，都会更新一次parameter，并且每个batch的数据也不同。因此，对于包含了非常多的噪声的数据，可能会导致错误的更新，导致无法收敛，无法拟合，所以这就需要采用更加高级的优化算法、采用正则化。

## 6.5.5 Symbol-to-Symbol Derivatives



## 6.5.6 General Back-Propagation



## 6.5.7 Example: Back-Propagation for MLP Training



## 6.5.8 Complications



## 6.5.9 Differentiation outside the Deep Learning Community

[automatic differentiation](http://en.wikipedia.org/wiki/Automatic_differentiation) 

reverse mode accumulation：

- https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation
- https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation