# 6.5 Back-Propagation and Other Differentiation Algorithms

When we use a feedforward neural network to accept an input $x$ and produce an output  $\hat{y}$, information flows forward through the network. The inputs $x$ provide the initial information that then propagates up to the hidden units at each layer and finally produces $\hat{y}$ . This is called **forward propagation** . During training,
**forward propagation** can continue onward until it produces a scalar cost $J( \theta )$. The back-propagation algorithm ( Rumelhart et al. 1986a ), often simply called  **backprop**, allows the information from the cost to then flow backwards through the network, in order to compute the gradient.

Computing an analytical expression for the gradient is straightforward, but numerically evaluating such an expression can be computationally expensive. The back-propagation algorithm does so using a simple and inexpensive procedure.

The term back-propagation is often misunderstood as meaning the whole learning algorithm for multi-layer neural networks. Actually, back-propagation refers only to the method for computing the **gradient**, while another algorithm, such as **stochastic gradient descent**, is used to perform learning using this **gradient**. Furthermore, back-propagation is often misunderstood as being specific to multi-layer neural networks, but in principle it can compute **derivatives** of any function (for some functions, the correct response is to report that the derivative of the function is undefined). Specifically, we will describe how to compute the **gradient** ∇ x f ( x y , ) for an arbitrary function f , where x is a set of variables whose derivatives are desired, and $y$ is an additional set of variables that are inputs to the function but whose derivatives are not required. In learning algorithms, the **gradient** we most often require is the gradient of the **cost function** with respect to the **parameters**, ∇ θ J ( θ ). Many machine learning tasks involve computing other derivatives, either as part of the learning process, or to analyze the learned model. The **back-propagation algorithm** can be applied to these tasks as well, and is not restricted to computing the **gradient** of the **cost function** with respect to the **parameters**. The idea of computing derivatives by propagating information through a network is very general, and can be used to compute values such as the **Jacobian** of a function f with multiple outputs. We restrict our description here to the most commonly used case where has a single output.

## 6.5.1 Computational Graphs

So far we have discussed neural networks with a relatively informal graph language. To describe the back-propagation algorithm more precisely, it is helpful to have a more precise language.

Many ways of formalizing computation as graphs are possible.

Here, we use each node in the graph to indicate a variable. The variable may be a scalar, vector, matrix, tensor, or even a variable of another type. To formalize our graphs, we also need to introduce the idea of an operation . An operation is a simple function of one or more variables. Our graph language is accompanied by a set of allowable operations. Functions more complicated than the operations in this set may be described by composing many operations together.

Without loss of generality, we define an operation to return only a single output variable. This does not lose generality because the output variable can have multiple entries, such as a vector. Software implementations of back-propagation usually support operations with multiple outputs, but we avoid this case in our description because it introduces many extra details that are not important to conceptual understanding. 

If a variable y is computed by applying an operation to a variable x , then we draw a directed edge from x to y . We sometimes annotate the output node with the name of the operation applied, and other times omit this label when the operation is clear from context.

Examples of computational graphs are shown in figure 6.8. 

![](./Figure-6.8.jpg)





> NOTE: 
>
> ### 思考: 如何根据函数表达式来构造computational graph
>
> 这个过程跟compiler parse我们编写的program是类似的，函数表达式和我们的program都是遵循grammar的，compiler根据grammar（往往使用production的方式来表达）使用grammar tree来表示我们的program；我们也可以根据function expression的grammar构造出它的computational graph，其实上述computational graph非常类似于abstract syntax tree的；在`Programming\software-TensorFlow\Implementation\TensorFlow-white-paper\Computation-graph-VS-parse-tree.md`中也对这个问题进行了分析。
>
> computational graph的构造构成是可以参考parsing的过程的。

## 6.5.2 Chain Rule of Calculus

The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions whose derivatives are known. Back-propagation is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient.

Let x be a real number, and let f and g both be functions mapping from a real number to a real number. Suppose that y = g ( x ) and z = f ( g ( x )) = f ( y ). Then the chain rule states that



We can generalize this beyond the scalar case. Suppose that x ∈ R m , y ∈ R n ,g maps from R m to R n , and f maps from R n to R . If y = g ( x ) and z = f ( y ), then



(6.45)

In vector notation, this may be equivalently written as

(6.46)

where ∂y/∂x is the Jacobian matrix of g.

> NOTE: 式(6.45)和式(6.46)，可以参见维基百科[Chain rule#General rule](https://en.wikipedia.org/wiki/Chain_rule#General_rule)

> NOTE: 式6.46是一个递归定义

From this we see that the gradient of a variable x can be obtained by multiplying a Jacobian matrix ∂y/∂x by a gradient ∇ y z . The back-propagation algorithm consists of performing such a Jacobian-gradient product for each operation in the graph.



Usually we do not apply the back-propagation algorithm merely to vectors, but rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the same as back-propagation with vectors. The only difference is how the numbers are arranged in a grid to form a tensor. We could imagine flattening each tensor into a vector before we run back-propagation, computing a vector-valued gradient, and then reshaping the gradient back into a tensor. In this rearranged view, back-propagation is still just multiplying Jacobians by gradients. 



To denote the gradient of a value z with respect to a tensor X , we write ∇ X z , just as if X were a vector. The indices into X now have multiple coordinates—for example, a 3-D tensor is indexed by three coordinates. We can abstract this away by using a single variable i to represent the complete tuple of indices. For all
possible index tuples i , ( ∇ X z ) i gives ∂z ∂X i . This is exactly the same as how for all possible integer indices i into a vector, ( ∇ x z ) i gives ∂z ∂x i . Using this notation, we can write the chain rule as it applies to tensors. If and , then



(6.47)

> NOTE: 式(6.47)和式(6.45)和式(6.46)，本质相同，只是表示形式不同而已

> NOTE: 上述式子仅仅给出了非常简单的复合函数，实际的full-connected network远比它要复杂。所以需要使用更加高级、更加general的描述方式：[Vector calculus](https://en.wikipedia.org/wiki/Vector_calculus)，所有的都要以[tensor](https://en.wikipedia.org/wiki/Tensor)为单位，与此相关的概念有：
>
> - [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)
>
> 



## 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop

Using the chain rule, it is straightforward to write down an algebraic expression for the gradient of a scalar with respect to any node in the computational graph that produced that scalar. However, actually evaluating that expression in a computer introduces some extra considerations.

Specifically, many subexpressions may be repeated several times within the overall expression for the gradient. Any procedure that computes the gradient will need to choose whether to store these subexpressions or to recompute them several times. An example of how these repeated subexpressions arise is given in figure 6.9.  In some cases, computing the same subexpression twice would simply be wasteful. For complicated graphs, there can be exponentially many of these wasted computations, making a naive implementation of the chain rule infeasible. In other cases, computing the same subexpression twice could be a valid way to reduce memory consumption at the cost of higher runtime.

> NOTE: tradeoff



We first begin by a version of the **back-propagation algorithm** that specifies the actual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the associated forward computation), in the order it will actually be done and according to the recursive application of chain rule. One could either directly perform these computations or view the description of the algorithm as a **symbolic specification**
of the **computational graph** for computing the back-propagation. However, this formulation does not make explicit the manipulation and the construction of the **symbolic graph** that performs the **gradient computation**. Such a formulation is presented below in section 6.5.6, with algorithm 6.5, where we also generalize to nodes that contain arbitrary tensors.



First consider a computational graph describing how to compute a single scalar $u^{(n)}$ (say the **loss** on a training example). This scalar is the quantity whose gradient we want to obtain, with respect to the $n_i$ input nodes $u^{(1)}$ to $u^{(n_i )}$ . In other words we wish to compute $ \frac { \partial u^{(n)} } { \partial u^{(i)} }$ for all $i \in { 1 , 2 ,...,n_i }$ . In the application of back-propagation to computing gradients for gradient descent over parameters, $u^{ ( n ) }$ will be the cost associated with an example or a minibatch, while $u^{(1)}$ to $u^{(n_i )}$ correspond to the parameters of the model.

> NOTE: 原文作者变量命名的方式是非常不易懂的。

We will assume that the nodes of the graph have been ordered in such a way that we can compute their output one after the other, starting at $u^{(n_i +1)}$ and going up to $u^{n}$. As defined in algorithm 6.1, each node $u ^{( i )}$  is associated with an operation $f^{( i )}$  and is computed by evaluating the function
$$
u^{(i)} = f(A^{(i)})   (6.48)
$$
where $A^{( i )}$  is the set of all nodes that are parents of $u^{(i)}$ .

> NOTE: 式(6.48)所描述的是前馈过程，即 forward propagation computation

![](./algorithm-6.1.jpg)



That algorithm specifies the **forward propagation computation**, which we could put in a graph $G$ . In order to perform back-propagation, we can construct a computational graph that depends on G and adds to it an extra set of nodes. These form a subgraph B with one node per node of G . Computation in B proceeds in exactly the reverse of the order of computation in G , and each node of B computes the derivative $\frac { \partial u^{(n)} } { \partial u^{(i)} }$associated with the forward graph node $u^{(i)}$ . This is done using the chain rule with respect to scalar output $u ^ { (n) }$ :





## 6.5.4 Back-Propagation Computation in Fully-Connected MLP



## 6.5.5 Symbol-to-Symbol Derivatives



## 6.5.6 General Back-Propagation



## 6.5.7 Example: Back-Propagation for MLP Training



## 6.5.8 Complications



## 6.5.9 Differentiation outside the Deep Learning Community

[automatic differentiation](http://en.wikipedia.org/wiki/Automatic_differentiation) 

reverse mode accumulation：

- https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation
- https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation