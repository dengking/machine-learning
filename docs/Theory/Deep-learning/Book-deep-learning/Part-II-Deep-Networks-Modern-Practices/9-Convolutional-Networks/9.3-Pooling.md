# 9.3 Pooling

A typical layer of a **convolutional network** consists of three stages (see figure 9.7).  In the first stage, the layer performs several convolutions in parallel to produce a set of **linear activations**. In the second stage, each **linear activation** is run through a **nonlinear activation function**, such as the **rectified linear activation function**. This stage is sometimes called the **detector** stage. In the third stage, we use a
**pooling function** to modify the output of the layer further.

A **pooling function** replaces the output of the net at a certain location with a **summary statistic** of the nearby outputs.  For example, the **max pooling** (Zhou and Chellappa, 1988 ) operation reports the **maximum output** within a **rectangular neighborhood**. Other popular pooling functions include the **average of a rectangular neighborhood**, the $L^2$ norm of a **rectangular neighborhood**, or a **weighted average** based on the distance from the central pixel.

In all cases, **pooling** helps to make the representation become approximately **invariant** to small translations of the input. **Invariance to translation** means that if we translate the input by a small amount, the values of most of the **pooled outputs** do not change（更加robust）. See figure 9.8 for an example of how this works.  *Invariance to local translation can be a very useful property if we care more about whether some feature is present than exactly where it is*（不关心位置，而仅仅关注这个feature是否出现）.For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy, we just need to know that there is an eye on the left side of the face and an eye on the right side of the face. In other contexts, it is more important to preserve the location of a feature. For example, if we want to find a corner defined by two edges meeting at a specific orientation, we need to preserve the location of the edges well enough to test whether they meet.

The use of **pooling** can be viewed as adding an infinitely strong prior that the function the layer learns must be **invariant** to small translations. When this assumption is correct, it can greatly improve the statistical efficiency of the network.

