# english
Decouple 解耦
# machine learning
## decision tree
The goal is to create a model that predicts the value of a target variable by learning simple **decision rules** inferred from the data features.

The deeper the tree, the more complex the decision rules and the fitter the model.往往是树越深，则decision rule越复杂，且拟合效果越好。

### 决策树的一些缺点disadvantage
- 容易过拟合

Decision-tree learners can create over-complex trees that do not generalise the data well. This is called **overfitting**. Mechanisms such as **pruning** (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.通过剪枝、限制每个叶子节点的sample的个数、树的深度来防止**overfitting**。
- 不稳定

Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated（减轻） by using decision trees within an ensemble.通过ensemble方法来减轻决策树的不稳定性

- 无法达到全局最优

The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on **heuristic algorithms** such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.决策树采用的是贪心算法，无法保证达到全局最优。可以通过使用ensemble方法来减轻此问题。
- 无法处理数据不均衡问题

Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.当某个类别的sample占比较高的时候，就会导致创建的树出现偏差，所以在拟合之前最好先balance一下dataset

### 对于连续型变量，决策树如何来进行划分？？
现在看来这个问题和下面的`in machine learning how to add discrete label to continuous data`有些接近，都是对连续型变量进行划分的。显然两者使用的算法是不相同的。


### Tips on practical use
- Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit.
- Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a better chance of finding features that are discriminative(有识别力的).
- Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant<!--在训练之前平衡数据集，以防止树偏向主导的类。-->. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.


## in machine learning how to add discrete label to continuous data
刚刚使用`in machine learning how to add discrete label to continuous data`搜索了一下，发现了很多相关的内容:

https://www.reddit.com/r/MachineLearning/comments/39ax98/converting_continuous_data_to_discrete_data_for/

https://www.analyticsvidhya.com/blog/2015/11/8-ways-deal-continuous-variables-predictive-modeling/

https://stackoverflow.com/questions/23267767/how-to-do-discretization-of-continuous-attributes-in-sklearn

## ==TODO== data normalisation

## ==TODO== dummy variables

## ==TODO== Running a notebook server
https://jupyter-notebook.readthedocs.io/en/stable/public_server.html

## Feature selection
http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection
# python
## pandas groupby transform的一个坑
分组，并用前值来补缺，慎用下面这种写法，会导致最后`self.data`中没有`CompanyCode`列
```
f = lambda x: x.fillna(method='pad')
self.data = self.data.groupby(['CompanyCode'])[self.local_table.fields].transform(f)
```
用下面这种写法：
```
self.data = self.data.groupby(['CompanyCode']).ffill()
```

## qcut
https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html