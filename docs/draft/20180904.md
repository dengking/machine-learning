# Reshaping and Pivot Tables
## Reshaping by pivoting DataFrame objects
stacked, record

![](https://pandas.pydata.org/pandas-docs/stable/_images/reshaping_pivot.png)

从图片看出，`df`中的数据存在如下规律：
- 列`bar`可取值`A`,`B`,`C`，且每个值的个数相同
- 列`foo`和列`bar`的取值是配套的

要想使用`pivot`，则原dataframe需要满足这些条件:
- 
# Select rows from a DataFrame based on values in a column in pandas
https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas

# PerformanceWarning: indexing past lexsort depth may impact performance.
在访问数据前先调用一下如下函数:

```
df.sort_index(inplace=True)
```
https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.sort_values.html

# ==TODO== pandas ValueError
报错信息如下：
```
ValueError                                Traceback (most recent call last)
ValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'
Exception ignored in: 'pandas._libs.lib.is_bool_array'
ValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'
```

# ==TODO== Reshaping and Pivot Tables
https://pandas.pydata.org/pandas-docs/stable/reshaping.html



# English
methodological  方法论

drastically 大幅地

confidence interval 置信区间

lasso 索套，拉拢

be amenable to 适用于

anova 方差分析

quad 四

religion 宗教，信仰

atheism 无神论

work around 解决

ambiguity 二义性

mimic 模仿

IOW in other word

compartmentalization 划分
# scikit-learn class hierarchy

|             |        作用        |  method    |
| :---------: | :----------------: | ---- |
|  estimator  |     base class     |   `fit`,`get_params`,`set_params`   |
| transformer | Preprocessing data |      |
|  predictor  |                    |      |
| classifier  |   classification   |      |
| regressor   | regression      |        |


```
graph TB
A[estimator]
B[transformer]-->|is a|A
C[predictor]-->|is a|A
D[regressor]-->|is a|A
```


# [使用Pipeline来combining estimators](http://scikit-learn.org/stable/modules/pipeline.html)
There is often a fixed sequence of steps in processing the data, for example **feature selection**, **normalization** and **classification**.So we can use Pipeline to chain multiple **estimators**(基类) into one. 

其中feature selection属于[Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)范轴

其中normalization则属于[Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html)范轴

classification则主要属于[Model selection and evaluation](http://scikit-learn.org/stable/model_selection.html)，目前包含如下内容：
- [Cross-validation: evaluating estimator performance](http://scikit-learn.org/stable/modules/cross_validation.html)

- [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)

## 如何使用`Pipeline`？
下面是一些具有启发性的例子
### [Sample pipeline for text feature extraction and evaluation](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)
第一个一个pipeline，这个pipeline的**last** **estimator**是一个Classifier
```
# Define a pipeline combining a text feature extractor with a simple
# classifier
pipeline = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', SGDClassifier()),
])
```
Pipeline具有这种[特性](http://scikit-learn.org/stable/modules/pipeline.html#notes):
>Calling `fit` on the pipeline is the same as calling `fit` on each **estimator** in turn, `transform` the input and pass it on to the next step. The pipeline has all the methods that the **last** **estimator** in the pipeline has, i.e. if the **last** **estimator** is a **classifier**, the Pipeline can be used as a **classifier**. If the **last** **estimator** is a transformer, again, so is the pipeline.

所以在下面这种用法中，此时的`pipline`可以看做是一个`classifier`。
```
    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
```

# [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html)
一个模型有两类参数：
- 学习参数learning parameters，从数据中学习
- 超参数hyperparameters，人为设置

显然**学习参数**是我们无法改变的，但是**超参数**是需要人为来进行调整的。learning parameters是从数据中学习的，我们人为无法进行更改，而hyperparameters则是需要我们手动地去寻找最优值的（这就是3.2. Tuning the hyper-parameters of an estimator内容），只有当这两类参数都确定下来，并且都达到最优，我们才算寻找到了一个完整的机器学习模型，然后才能够使用测试集进行验证。

对于这两类参数，我们都是需要对它们进行评估的，这就涉及到如何进行评估（这就是cross validation所做的工作），并且对他们的评估也决定了我们能否找到更好的解。

当我们执行完一轮学习和调参后，也就是一个模型的参数最终确认了，就可以使用测试集来对整个模型的性能进行验证。

使用一个训练集来训练出一个模型，然后调整超参数，validate whether a hyperparameter is good。

有一点是可以肯定的：不能够使用测测试集取验证一个超参数的值是否是好的，如果使用测试集来进行验证的话，则knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance.为了解决这个问题，就提出了validation set的概念，即使用validation set来验证某个超参数的好坏。

因此，最终数据集会成为如下三个方面：
- training set，用于学习learning parameters
- validation set，用于验证此hyperparameters的值是否好
- test set

但是如果这样来进行划分的话，则会导致we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.

为此就引入了[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))， CV的思想如下：    

A **test set** should still be held out for final evaluation, but the **validation set** is no longer needed when doing **CV**. In the basic approach, called **k-fold CV**, the **training set** is split into **k** smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:
- A model is trained using `k-1` of the folds as **training data**;
- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as **accuracy**).


显然在cross validation中，数据集会被分为如下两类：
- training set
- test set

从[这个例子](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)中是可以看出来的



==Q&A==
- k-fold CV是训练一个明细k次还是训练k次产生k个模型
- `cross_val_score`所输出的score是用test set评估的还是使用的训练集中的第k折数据进行评估的？

    根据前面的阅读来看，肯定不是test set来评估的，应该是使用第k折数据进行评估的
    
# [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)
tuning the hyper-parameters的更加准确的描述是([原文链接](http://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator))：
> search the hyper-parameter space for the best cross validation score.

即在超参数空间中寻找到最佳的[cross validation](http://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator) score.

如下是scikit-learn所提供的search的[组成要素](http://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator):
- an estimator (regressor or classifier such as `sklearn.svm.SVC()`);
- a parameter space;
- a method for searching or sampling candidates;
- a cross-validation scheme; and
- a [score function](http://scikit-learn.org/stable/modules/grid_search.html#gridsearch-scoring).

两个通用的sampling search candidates方法如下：
- [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)
- [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)

[最佳实践](http://scikit-learn.org/stable/modules/grid_search.html#tips-for-parameter-search)

[这个例子](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)符合最佳实践的[Model selection: development and evaluation](http://scikit-learn.org/stable/modules/grid_search.html#model-selection-development-and-evaluation)原则。
## RandomizedSearchCV
[RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) can sample a given number of candidates from a parameter space with a **specified distribution**.

==Q&A== 
- 此处的**specified distribution**如何理解
    

## 使用和不使用GridSearchCV的对比

[不使用的例子](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py)

[使用的例子](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)

