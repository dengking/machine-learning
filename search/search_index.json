{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b # \u672c\u5de5\u7a0b\u6309\u7167\u7406\u8bba\u4e0e\u5b9e\u8df5\u76f8\u7ed3\u5408\u7684\u601d\u8def\u6765\u603b\u7ed3machine learning\u7684\u77e5\u8bc6\u3002","title":"Home"},{"location":"#_1","text":"\u672c\u5de5\u7a0b\u6309\u7167\u7406\u8bba\u4e0e\u5b9e\u8df5\u76f8\u7ed3\u5408\u7684\u601d\u8def\u6765\u603b\u7ed3machine learning\u7684\u77e5\u8bc6\u3002","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"AI-meeting/","text":"\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u9876\u7ea7\u4f1a\u8bae # Conference on Neural Information Processing Systems # NIPS International Conference on Computer Vision # ICCV Conference on Computer Vision and Pattern Recognition # CVPR International Conference on Learning Representations # ICLR","title":"AI-meeting"},{"location":"AI-meeting/#_1","text":"","title":"\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u9876\u7ea7\u4f1a\u8bae"},{"location":"AI-meeting/#conference-on-neural-information-processing-systems","text":"NIPS","title":"Conference on Neural Information Processing Systems"},{"location":"AI-meeting/#international-conference-on-computer-vision","text":"ICCV","title":"International Conference on Computer Vision"},{"location":"AI-meeting/#conference-on-computer-vision-and-pattern-recognition","text":"CVPR","title":"Conference on Computer Vision and Pattern Recognition"},{"location":"AI-meeting/#international-conference-on-learning-representations","text":"ICLR","title":"International Conference on Learning Representations"},{"location":"AI-papers/","text":"\u8bba\u6587 # \u54ea\u91cc\u53bb\u627e\u8bba\u6587\uff1f\u54ea\u91cc\u53bb\u627e\u8fd9\u4e9b\u8bba\u6587\u7684\u5b9e\u73b0\uff1f paper with code # \u7ed9\u51fa\u4e86\u8bba\u6587\u7684\u5b9e\u73b0\u3002 \u7f8e\u56fd\u5b66\u672f\u8bba\u6587\u3001\u79d1\u5b66\u7814\u7a76\u7c7b\u7f51\u7ad9\u4f7f\u7528\u63a8\u8350 # \u7c7b\u4f3c\u4e2d\u56fd\u77e5\u7f51\u4f46\u662f\u641c\u7d22\u82f1\u6587\u6587\u732e\u7684\u6743\u5a01\u7f51\u7ad9\u6709\u54ea\u4e9b\uff1f #","title":"AI-papers"},{"location":"AI-papers/#_1","text":"\u54ea\u91cc\u53bb\u627e\u8bba\u6587\uff1f\u54ea\u91cc\u53bb\u627e\u8fd9\u4e9b\u8bba\u6587\u7684\u5b9e\u73b0\uff1f","title":"\u8bba\u6587"},{"location":"AI-papers/#paper-with-code","text":"\u7ed9\u51fa\u4e86\u8bba\u6587\u7684\u5b9e\u73b0\u3002","title":"paper with code"},{"location":"AI-papers/#_2","text":"","title":"\u7f8e\u56fd\u5b66\u672f\u8bba\u6587\u3001\u79d1\u5b66\u7814\u7a76\u7c7b\u7f51\u7ad9\u4f7f\u7528\u63a8\u8350"},{"location":"AI-papers/#_3","text":"","title":"\u7c7b\u4f3c\u4e2d\u56fd\u77e5\u7f51\u4f46\u662f\u641c\u7d22\u82f1\u6587\u6587\u732e\u7684\u6743\u5a01\u7f51\u7ad9\u6709\u54ea\u4e9b\uff1f"},{"location":"TODO/","text":"AI # Regularization # https://en.wikipedia.org/wiki/Regularization_(mathematics) https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/ dropout # https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/ https://stackoverflow.com/questions/35545798/keep-prob-in-tensorflow-mnist-tutorial TODO # Machine learning # Graphical model # Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Helmholtz machine # restricted Boltzmann machine http://cs230.stanford.edu/blog/createtrainmodel/ https://danijar.com/patterns-for-fast-prototyping-with-tensorflow/ https://hackernoon.com/towards-ai-how-long-does-it-take-you-to-go-from-idea-to-working-prototype-a-day-a-month-8a03ffecca0a tf.contrib.layers.xavier_initializer # https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/xavier_initializer?hl=cy https://stackoverflow.com/questions/42286426/what-is-the-difference-between-xavier-initializer-and-xavier-initializer-conv2d?rq=1 position embedding # https://stackoverflow.com/questions/44614603/what-is-the-the-position-embedding-in-the-convolutional-sequence-to-sequence-lea time serial forecast # https://www.tensorflow.org/tutorials/structured_data/time_series attention # self attention # Self-Attention Mechanisms in Natural Language Processing transformer # https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/ attention and model architecture # \u76ee\u524d\u6240\u9605\u8bfb\u7684\u5982\u4e0b\u8bba\u6587\u4e2d\uff0c\u4f7f\u7528attention\u7684model architecture\u90fd\u662fencoder-decoder 201409 Neural Machine Translation by Jointly Learning to Align and Translate 201502 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention \u5f53\u7136\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u6709\u8bba\u6587\u4e00\u79cdarchitecture\uff1atransformer\uff0c\u90a3transformer\u4e2d\u662f\u5982\u4f55\u7684\u5462\uff1f\u8fd8\u9700\u8981\u9605\u8bfb\u5b83\u7684paper\uff1b drop-out # \u6784\u9020\u56fe\u7684\u8fc7\u7a0b\u548cFutures and promises\u4e4b\u95f4\u7684\u5173\u7cfb # https://en.wikipedia.org/wiki/Futures_and_promises tensorflow\u7684\u6d4b\u8bd5 # \u5728\u642d\u5efa\u540e\u6765computation graph\u540e\uff0c\u662f\u53ef\u4ee5\u4f7f\u7528numpy\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u7684\uff0c\u5982\uff1a https://stackoverflow.com/questions/51443898/how-do-i-use-tf-reshape how to find tensorflow bottleneck # \u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684AI\u5e94\u7528 # \u6211\u76ee\u524d\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u8fd8\u6ca1\u6709\u72ec\u7acb\u5730\u5b8c\u6210\u4e00\u4e2aAI\u4efb\u52a1\uff0c\u9700\u8981\u627e\u65f6\u95f4\u6765\u7ec3\u4e60\u3002 use knowledge graph to enhance semantic comprehension # https://www.aminer.cn/research_report/5dd163d61ca12517163f032c?download=false \u6765\u52302019\u5e74\u7684\u4eca\u5929\uff0c\u6df1\u5ea6\u5b66\u4e60\u7684\u8bf8\u591a\u5c40\u9650\u6027\u4e5f\u6162\u6162\u5f97\u5230\u5e7f\u6cdb\u8ba4\u77e5\u3002\u5bf9\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u800c\u8a00\uff0c\u8981\u505a\u5230\u7cbe\u7ec6\u6df1\u5ea6\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5355\u7eaf\u4f9d\u9760\u6570\u636e\u6807\u6ce8\u4e0e\u7b97\u529b\u6295\u5165\u65e0\u6cd5\u89e3\u51b3\u672c\u8d28\u95ee\u9898\u3002\u5982\u679c\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u7684\u652f\u6301\uff0c\u201c\u4e2d\u56fd\u7684\u4e52\u4e53\u7403\u8c01\u90fd\u6253\u4e0d\u8fc7\u201d\u4e0e\u201c\u4e2d\u56fd\u7684\u8db3\u7403\u8c01\u90fd\u6253\u4e0d\u8fc7\u201d\uff0c\u5728\u8ba1\u7b97\u673a\u770b\u6765\u8bed\u4e49\u4e0a\u5e76\u6ca1\u6709\u5de8\u5927\u5dee\u5f02\uff0c\u800c\u5b9e\u9645\u4e0a\u4e24\u53e5\u4e2d\u7684\u201c\u6253\u4e0d\u8fc7\u201d\u610f\u601d\u6b63\u597d\u76f8\u53cd\u3002\u56e0\u6b64\uff0c\u878d\u5165\u77e5\u8bc6\u6765\u8fdb\u884c\u77e5\u8bc6\u6307\u5bfc\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u662f\u901a\u5411\u7cbe\u7ec6\u800c\u6df1\u5ea6\u7684\u8bed\u8a00\u7406\u89e3\u7684\u5fc5\u7531\u4e4b\u8def\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u53c8\u4ece\u54ea\u91cc\u6765\u5462\uff1f\u8fd9\u5c31\u6d89\u53ca\u5230\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898\u2014\u2014\u77e5\u8bc6\u83b7\u53d6\u3002 knowledge graph # https://www.zhihu.com/topic/19551341/hot https://www.zhihu.com/org/hua-wei-yun-ji-zhu-zhai-ji-di/activities https://www.zhihu.com/people/ji-yi-chao/activities https://www.zhihu.com/question/354059866/answer/907009362 https://www.zhihu.com/topic/19838204/hot","title":"AI"},{"location":"TODO/#ai","text":"","title":"AI"},{"location":"TODO/#regularization","text":"https://en.wikipedia.org/wiki/Regularization_(mathematics) https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/","title":"Regularization"},{"location":"TODO/#dropout","text":"https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/ https://stackoverflow.com/questions/35545798/keep-prob-in-tensorflow-mnist-tutorial","title":"dropout"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#machine-learning","text":"","title":"Machine learning"},{"location":"TODO/#graphical-model","text":"Structured prediction Graphical models Bayes net Conditional random field Hidden Markov","title":"Graphical model"},{"location":"TODO/#helmholtz-machine","text":"restricted Boltzmann machine http://cs230.stanford.edu/blog/createtrainmodel/ https://danijar.com/patterns-for-fast-prototyping-with-tensorflow/ https://hackernoon.com/towards-ai-how-long-does-it-take-you-to-go-from-idea-to-working-prototype-a-day-a-month-8a03ffecca0a","title":"Helmholtz machine"},{"location":"TODO/#tfcontriblayersxavier_initializer","text":"https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/layers/xavier_initializer?hl=cy https://stackoverflow.com/questions/42286426/what-is-the-difference-between-xavier-initializer-and-xavier-initializer-conv2d?rq=1","title":"tf.contrib.layers.xavier_initializer"},{"location":"TODO/#position-embedding","text":"https://stackoverflow.com/questions/44614603/what-is-the-the-position-embedding-in-the-convolutional-sequence-to-sequence-lea","title":"position embedding"},{"location":"TODO/#time-serial-forecast","text":"https://www.tensorflow.org/tutorials/structured_data/time_series","title":"time serial forecast"},{"location":"TODO/#attention","text":"","title":"attention"},{"location":"TODO/#self-attention","text":"Self-Attention Mechanisms in Natural Language Processing","title":"self attention"},{"location":"TODO/#transformer","text":"https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/","title":"transformer"},{"location":"TODO/#attention-and-model-architecture","text":"\u76ee\u524d\u6240\u9605\u8bfb\u7684\u5982\u4e0b\u8bba\u6587\u4e2d\uff0c\u4f7f\u7528attention\u7684model architecture\u90fd\u662fencoder-decoder 201409 Neural Machine Translation by Jointly Learning to Align and Translate 201502 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention \u5f53\u7136\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u6709\u8bba\u6587\u4e00\u79cdarchitecture\uff1atransformer\uff0c\u90a3transformer\u4e2d\u662f\u5982\u4f55\u7684\u5462\uff1f\u8fd8\u9700\u8981\u9605\u8bfb\u5b83\u7684paper\uff1b","title":"attention and model architecture"},{"location":"TODO/#drop-out","text":"","title":"drop-out"},{"location":"TODO/#futures-and-promises","text":"https://en.wikipedia.org/wiki/Futures_and_promises","title":"\u6784\u9020\u56fe\u7684\u8fc7\u7a0b\u548cFutures and promises\u4e4b\u95f4\u7684\u5173\u7cfb"},{"location":"TODO/#tensorflow","text":"\u5728\u642d\u5efa\u540e\u6765computation graph\u540e\uff0c\u662f\u53ef\u4ee5\u4f7f\u7528numpy\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u7684\uff0c\u5982\uff1a https://stackoverflow.com/questions/51443898/how-do-i-use-tf-reshape","title":"tensorflow\u7684\u6d4b\u8bd5"},{"location":"TODO/#how-to-find-tensorflow-bottleneck","text":"","title":"how to find tensorflow bottleneck"},{"location":"TODO/#ai_1","text":"\u6211\u76ee\u524d\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u8fd8\u6ca1\u6709\u72ec\u7acb\u5730\u5b8c\u6210\u4e00\u4e2aAI\u4efb\u52a1\uff0c\u9700\u8981\u627e\u65f6\u95f4\u6765\u7ec3\u4e60\u3002","title":"\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684AI\u5e94\u7528"},{"location":"TODO/#use-knowledge-graph-to-enhance-semantic-comprehension","text":"https://www.aminer.cn/research_report/5dd163d61ca12517163f032c?download=false \u6765\u52302019\u5e74\u7684\u4eca\u5929\uff0c\u6df1\u5ea6\u5b66\u4e60\u7684\u8bf8\u591a\u5c40\u9650\u6027\u4e5f\u6162\u6162\u5f97\u5230\u5e7f\u6cdb\u8ba4\u77e5\u3002\u5bf9\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u800c\u8a00\uff0c\u8981\u505a\u5230\u7cbe\u7ec6\u6df1\u5ea6\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5355\u7eaf\u4f9d\u9760\u6570\u636e\u6807\u6ce8\u4e0e\u7b97\u529b\u6295\u5165\u65e0\u6cd5\u89e3\u51b3\u672c\u8d28\u95ee\u9898\u3002\u5982\u679c\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u7684\u652f\u6301\uff0c\u201c\u4e2d\u56fd\u7684\u4e52\u4e53\u7403\u8c01\u90fd\u6253\u4e0d\u8fc7\u201d\u4e0e\u201c\u4e2d\u56fd\u7684\u8db3\u7403\u8c01\u90fd\u6253\u4e0d\u8fc7\u201d\uff0c\u5728\u8ba1\u7b97\u673a\u770b\u6765\u8bed\u4e49\u4e0a\u5e76\u6ca1\u6709\u5de8\u5927\u5dee\u5f02\uff0c\u800c\u5b9e\u9645\u4e0a\u4e24\u53e5\u4e2d\u7684\u201c\u6253\u4e0d\u8fc7\u201d\u610f\u601d\u6b63\u597d\u76f8\u53cd\u3002\u56e0\u6b64\uff0c\u878d\u5165\u77e5\u8bc6\u6765\u8fdb\u884c\u77e5\u8bc6\u6307\u5bfc\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u662f\u901a\u5411\u7cbe\u7ec6\u800c\u6df1\u5ea6\u7684\u8bed\u8a00\u7406\u89e3\u7684\u5fc5\u7531\u4e4b\u8def\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u53c8\u4ece\u54ea\u91cc\u6765\u5462\uff1f\u8fd9\u5c31\u6d89\u53ca\u5230\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898\u2014\u2014\u77e5\u8bc6\u83b7\u53d6\u3002","title":"use knowledge graph  to enhance semantic comprehension"},{"location":"TODO/#knowledge-graph","text":"https://www.zhihu.com/topic/19551341/hot https://www.zhihu.com/org/hua-wei-yun-ji-zhu-zhai-ji-di/activities https://www.zhihu.com/people/ji-yi-chao/activities https://www.zhihu.com/question/354059866/answer/907009362 https://www.zhihu.com/topic/19838204/hot","title":"knowledge graph"},{"location":"Application/NLP/","text":"NLP # \u53c2\u89c1\u5de5\u7a0b NLP \u3002","title":"NLP"},{"location":"Application/NLP/#nlp","text":"\u53c2\u89c1\u5de5\u7a0b NLP \u3002","title":"NLP"},{"location":"Application/Computable-knowledge/Knowledge-representation-and-reasoning/","text":"Knowledge representation and reasoning #","title":"Introduction"},{"location":"Application/Computable-knowledge/Knowledge-representation-and-reasoning/#knowledge-representation-and-reasoning","text":"","title":"Knowledge representation and reasoning"},{"location":"Data-set/","text":"\u4ece\u4e8b\u673a\u5668\u5b66\u4e60\uff0c\u4e0d\u80fd\u591f\u7f3a\u5c11\u6570\u636e\u7684\u652f\u6301\uff0c\u8fd9\u91cc\u624b\u673a\u4e86\u4e00\u4e9b\u63d0\u4f9b\u4e86\u8f83\u597d\u7684\u6570\u636e\u96c6\u7684\u7f51\u7ad9\u3002 https://www.tensorflow.org/tutorials/keras/basic_regressionon","title":"Introduction"},{"location":"Data-set/Auto-MPG-Data-Set/AUTO-MPG-example/","text":"","title":"AUTO MPG example"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/","text":"\u524d\u8a00 model layer computational graph MLP CNN RNN LSTM Bi-RNN encoder-decoder/seq2seq encoder-align model-decoder \u524d\u8a00 # model # \u76ee\u524d\u5df2\u7ecf\u4f7f\u7528\u4e86\u4e09\u79cdmodel\uff1a MLP CNN RNN \u6bcf\u79cdmodel\u90fd\u8868\u793a\u4e00\u79cdcomputation\uff08\u8fd9\u662fmodel\u7684\u672c\u8d28\uff09\uff0c\u5728\u8bba\u6587\u4e2d\uff0c\u4e5f\u662f\u901a\u8fc7computation\uff08\u5404\u79cd\u5404\u6837\u7684\u516c\u5f0f\uff09\u6765\u63cf\u8ff0\u4e00\u4e2a\u6a21\u578b\u7684\uff1b\u800c\u5728\u5404\u79cdblog\u548ctutorial\u4e2d\uff0c\u5f80\u5f80\u559c\u6b22\u4f7f\u7528model\u7684topology\u6765\u4ecb\u7ecd\u5b83\uff1b\u663e\u7136model\u7684topology\u53ef\u4ee5\u770b\u505a\u662f\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u8868\u793a\uff0c\u5b83\u4eec\u80fd\u591f\u5e2e\u52a9\u5b66\u4e60\u8005\u66f4\u5feb\u5730\u5b66\u4e60model\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u7406\u89e3model\u7684computation\uff1b \u5728deep learning book\u76846.5.1 Computational Graphs\u4e2d\u4ecb\u7ecd\uff0c\u6211\u4eec\u53ef\u4ee5computationaln graph\u6765formalizing computation as graph\u3002\u5728TensorFlow\u7684low API\u4e2d\u6307\u51fa\uff1aTensorFlow\u4e5f\u662f\u4f7f\u7528 computational graph \u6765\u5b9e\u73b0\u5e95\u5c42\u8ba1\u7b97\u7684\u63cf\u8ff0\u7684\uff08 A TensorFlow computation, represented as a dataflow graph. tensorflow\u4e2d\u662f\u4f7f\u7528\u7684dataflow graph\u6765\u8868\u793acomputation\u7684\uff09 \u8981\u60f3\u5b8c\u6574\u5730\u638c\u63e1\u4e00\u4e2a\u6a21\u578b\uff0c\u9700\u8981\u4ececomputation\uff0cmodel topology\uff0ccomputation graph\u8fd9\u4e09\u4e2a\u65b9\u9762\u5165\u624b\uff1b\u5176\u5b9ecomputation\u624d\u662f\u672c\u8d28\u6240\u5728\uff0cmodel topology\uff0ccomputation graph\u90fd\u662f\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff1b\u9664\u6b64\u4e4b\u5916\uff0c\u76ee\u524d\u5927\u591a\u6570deep learning framework\u7684\u5b9e\u73b0\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8etensor\u7684\u6570\u636e\u8868\u793a\u3001\u57fa\u4e8etensor\u7684computation\uff0c\u4ece\u6574\u4f53\u6765\u770b\u8fd9\u4e2amodel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a\u51fd\u6570 f(input_tensor)= outout_tensor \uff0c input_tensor \u6d41\u7ecfmodel\u5f97\u5230 output_tensor \uff0c\u6240\u4ee5\u638c\u63e1tensor\u5728model\u4e2d\u7684\u6d41\u52a8\u8fc7\u7a0b\uff08\u6d41\u52a8\u8fc7\u7a0b\u4e2dtensor\u7684shape\u7684\u53d8\u5316\u7b49\uff09\u4e5f\u662f\u638c\u63e1model\u7684computation\u7684\u4e00\u4e2a\u6377\u5f84\uff1b\u66f4\u52a0\u51c6\u786e\u5730\u8bf4\u4ed6\u4eec\u662f\u4f7f\u7528 dataflow graph \u6765\u8868\u793acomputation\uff0c\u6bd4\u5982tensorflow\uff1b \u5728\u6a21\u578b\u8bbe\u8ba1\u4e2d\u6240\u8bbe\u8ba1\u5730\u5404\u79cd\u601d\u60f3\uff0c\u5982parameter sharing\u7b49\uff0c\u90fd\u662f\u6709\u5bf9\u5e94\u7684computation\u7684\uff1b Neural Networks, Manifolds, and Topology layer # \u6309\u7167\u76ee\u524d\u7684\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u5982tensorflow\uff0cKeras\uff0c\u90fd\u662f\u5c06model\u62bd\u8c61\u6210\u7531\u591a\u4e2alayer\u53e0\u52a0\u800c\u6210\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u662f\u5148\u4ece\u5b9e\u73b0\u5c42\u5165\u624b\uff1b\u5e76\u4e14\u5f88\u591a\u7684\u8bba\u6587\u4e2d\u4e5f\u662f\u8fd9\u6837\u63cf\u8ff0\u7684\uff1b\u4ecedeep learning book\u7684chapter 6\u4e2d\uff0c\u4e5f\u5c06\u8fd9\u4e2amodel\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u590d\u5408\u51fd\u6570\uff0c\u590d\u5408\u51fd\u6570\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5bf9\u5e94\u4e86\u4e00\u5c42\uff1b computational graph # \u5176\u5b9ecomputation graph\u5e76\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u5728deep learning book\u4e2d\u7684computation graph\u5c31\u548cTensorFlow\u7684computation graph\u662f\u5b9a\u4e49\u5c31\u662f\u4e0d\u540c\u7684\uff1b deep learning book\u4e2d\u7684computation graph\u4fa7\u91cd\u70b9\u5728\u4e8e\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff0c\u5b83\u7684\u89c4\u5219\u5982\u4e0b\uff1a - node in the graph to indicate a variable TensorFlow\u7684 computational graph \u7684\u5b9a\u4e49\u5982\u4e0b\uff1a tf.Operation (or \"ops\"): The nodes of the graph. Operations describe calculations that consume and produce tensors. tf.Tensor : The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors . \u663e\u7136\uff0cTensorFlow\u4e2d\u7684\u5b9a\u4e49\u548cdeep learning book\u4e2d\u7684\u5b9a\u4e49\u662f\u4e0d\u540c\u7684\uff0c\u76f4\u89c2\u662f\u89c9\u5f97TensorFlow\u4e2d computational graph \u7684\u5b9a\u4e49\u662f\u6bd4\u8f83\u9002\u5408\u4e8e\u7f16\u7801\u5b9e\u73b0\u7684\uff0c\u800cdeep learning book\u4e2d\u7684\u5b9a\u4e49\u662f\u4fbf\u4e8e\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff08\u5f53\u7136TensorFlow\u7684computational graph\u4e5f\u80fd\u591f\u8fbe\u5230\u8fd9\u4e2a\u76ee\u7684\uff09\uff1b \u5728\u4e0b\u9762\u7684\u63cf\u8ff0\u4e2d\uff0ccomputational graph\u7684\u63cf\u8ff0\u90fd\u662f\u6309\u7167TensorFlow\u4e2d\u7684\u5b9a\u4e49\uff1b MLP # MLP\u7684model topology\uff1a full connected\uff08\u8f93\u5165\u5c42\u4e0e\u9690\u85cf\u5c42\u4e4b\u95f4\uff0c\u9690\u85cf\u5c42\u4e0e\u9690\u85cf\u5c42\u4e4b\u95f4\uff0c\u9690\u85cf\u5c42\u4e0e\u8f93\u51fa\u5c42\u4e4b\u95f4\uff0c\u90fd\u662f\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u8fde\u63a5\u7684\uff09 MLP\u7684computation\uff1a matrix multiplication \u4e0b\u9762\u7684\u4ee3\u7801\u662f\u6458\u81ea Getting started with the Keras Sequential model \u7684Multilayer Perceptron (MLP) for multi-class softmax classification: import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u601d\u8003\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6240\u6784\u5efa\u7684\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u662f\u4ec0\u4e48\uff1f \u7b54\uff1a\u4e00\u822c \u8f93\u5165\u77e9\u9635 \u7684shape\u662f [batch_size, feature_num] \uff0c\u5219\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f [feature_num, hidden_layer_node_num_1] \uff0c\u5373\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\u548c batch_size \u65e0\u5173\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u7279\u6027\uff1a\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u4e0d\u77e5\u9053 batch_size \u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u4ee5\u6784\u5efa\u6a21\u578b\u4e86\uff0c\u65e0\u8bba\u5bf9\u4e8e\u4ec0\u4e48\u6a21\u578b\uff0c\u6a21\u578b\u4e2d\u6bcf\u4e00\u5c42\u7684 \u8282\u70b9\u6570 \u548c \u7279\u5f81\u7684\u4e2a\u6570 \u3001 batch_size \u90fd\u662f\u6ca1\u6709\u5173\u8054\u7684\uff08\u5f53\u6d89\u53caLSTM\u7684\u65f6\u5019\uff0cLSTM\u4e2d\u6bcf\u4e00\u5c42\u7684neuron\u7684\u4e2a\u6570\u548c time_step \u4e5f\u662f\u6ca1\u6709\u5173\u8054\u7684\uff09\uff0c \u7279\u5f81\u7684\u4e2a\u6570 \u4f1a\u5f71\u54cdneuron\u4e2d\u7684\u53c2\u6570\u4e2a\u6570\u6709\u5173\uff1b \u7531\u4e8eMLP\u8981\u6c42full connected\uff0c\u4f7f\u7528 \u77e9\u9635\u4e58\u6cd5 \u662f\u80fd\u591f\u975e\u5e38\u597d\u5730\u5b9e\u73b0\u8fd9\u79cd\u9700\u6c42\u7684\uff0c\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff1a [ [1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1], ] 4*3 \u7b2c\u4e00\u9690\u85cf\u5c42\u670910\u4e2anode\uff0c\u5219\u5b83\u7684\u6743\u91cd\u77e9\u9635\u662f[3 * 10] [ [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], ] \u6bcf\u4e00\u5217\u8868\u793a\u7684\u662f \u7b2c\u4e8c\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\uff1a [hidden_layer_node_num_1, hidden_layer_node_num_2] \uff0c\u4f9d\u6b21\u7c7b\u63a8\uff0c\u6240\u4ee5\u6700\u7ec8\u6700\u540e\u4e00\u5c42\u5373\u8f93\u51fa\u5c42\u7684\u4e0e\u524d\u4e00\u5c42\u4e4b\u95f4\u7684\u6743\u91cd\u77e9\u9635 [hidden_layer_node_num_-1, n_class] \uff08 -1 \u8868\u793a\u6700\u540e\u4e00\u5c42\uff09\u3002 \u6240\u4ee5\uff0c\u4e00\u4e2abatch_size\u7684\u6570\u636e\u6d41\u7ecfMLP\u4e4b\u540e\uff0c\u6700\u7ec8\u5f97\u5230\u7684\u6570\u636e\u7684shape\u662f [batch_size, n_classes] \u3002 \u5176\u5b9e\u4ece\u8fd9\u4e2a\u6570\u5b66\u5173\u7cfb\u4e5f\u53ef\u4ee5\u770b\u51fa\u4e3a\u4ec0\u4e48\u8981\u5c06label\u4ee5one-hot\u7684\u65b9\u5f0f\u8868\u793a\u4e86\uff1b \u4e0b\u9762\u7684\u4ee3\u7801\u662fTensorFlow\u4e2d\u6784\u5efaMLP\u7684\u4e00\u4e2ademo\uff1a with tf.name_scope('input'): self.x_in = tf.placeholder(tf.float32, [None, self.feature_num], name='x_in') # self.y_in = tf.placeholder(tf.float32, [None, self.time_step, self.n_classes], name='y_in') # \u6bcf\u4e2asequence\u90fd\u53d6\u6700\u540e\u4e00\u6761\u8bb0\u5f55\u7684target\u6765\u4f5c\u4e3a\u8fd9\u4e2asequence\u7684target self.y_in = tf.placeholder(tf.float32, [None, self.n_classes], name='y_in') self.keep_prob = tf.placeholder(tf.float32, name='dropout_in') with tf.name_scope('layer1'): w_in = self.__weight_variable__([self.feature_num, self.num_lstm_units]) b_in = self.__bias_variable__([self.num_lstm_units]) lstm_input_layer = tf.reshape(self.x_in, [-1, self.feature_num]) # input layer lstm_input_layer = tf.nn.relu(tf.matmul(lstm_input_layer, w_in) + b_in) \u4e00\u822c\uff0c\u6211\u4eec\u5728\u9605\u8bfb\u4e66\u7c4d\u7684\u65f6\u5019\uff0c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u6d41\u7a0b\u90fd\u662f\u4e00\u6b21\u8f93\u5165\u4e00\u6761\u8bb0\u5f55\uff0c\u8fd9\u79cd\u505a\u6cd5\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5b9e\u9645\u4e0a\u5982\u679c\u771f\u6ef4\u4e00\u6b21\u4ec5\u4ec5\u5582\u5165\u4e00\u6761\u6570\u636e\u7684\u8bdd\uff0c\u4f1a\u975e\u5e38\u7f13\u6162\uff1b\u5b9e\u9645\u7684\u5b9e\u73b0\u662f\u4e00\u6b21\u5582\u5165\u4e00\u4e2abatch\u7684\uff0c\u5373\u662f\u4e0a\u9762\u6240\u63cf\u8ff0\u7684 \u8f93\u5165\u77e9\u9635 \uff0c\u73b0\u4ee3\u7684GPU\u5904\u7406\u77e9\u9635\u8fd0\u7b97\u7684\u901f\u5ea6\u975e\u5e38\u5feb\uff1b\u5176\u5b9e\u4e00\u6b21\u5582\u5165\u4e00\u6761\u8bb0\u5f55\u4e5f\u53ef\u4ee5\u5957\u7528\u4e0a\u9762\u7684\u77e9\u9635\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5373 batch_size=1 \uff1b \u4eceTensorFlow\u7684\u4ee3\u7801\u53ef\u4ee5\u770b\u51fa\uff0c\u8f93\u5165\u77e9\u9635\u548c\u7b2c\u4e00\u5c42\u7684\u6743\u91cd\u77e9\u9635\u6267\u884c\u77e9\u9635\u4e58\u6cd5\uff0c\u6839\u636e\u77e9\u9635 \u4e58\u6cd5\u539f\u7406 \u53ef\u4ee5\u77e5\u9053\u6bcf\u4e00\u6761\u6570\u636e\u4f1a\u6d41\u5165\u5230\u7b2c\u4e00\u9690\u85cf\u5c42\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u6761\u8bb0\u5f55\u6d41\u5165\u4e00\u4e2a\u8282\u70b9\u4ea7\u751f\u7684\u8f93\u51fa\u5176\u5b9e\u662f\u4e00\u4e2a\u6807\u91cf\uff1b\u5176\u5b9e\u8fd9\u4e5f\u662ffull connected\u7684\u542b\u4e49\u6240\u5728\uff1b CNN # \u5173\u4e8eCNN\u7684computation\u548cmodel topology\u53c2\u8003\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u975e\u5e38\u597d\u7684\u89e3\u91ca\uff1a Convolutional Neural Networks (CNNs / ConvNets) A Beginner's Guide To Understanding Convolutional Neural Networks \u5168\u8fde\u63a5\u5bf9\u5e94\u7684\u662f\u77e9\u9635\u4e58\u6cd5\uff0cCNN\u4e2d\u7684filter\u5219\u5bf9\u5e94\u7684\u5377\u79ef\u8fd0\u7b97\uff0c\u5377\u79ef\u5c42\u4e2d\u7684\u795e\u7ecf\u5143\u53ea\u4f1a\u548cinput\u7684\u4e00\u90e8\u5206\u8fdb\u884c\u8fde\u63a5\uff0c\u800c\u4e0d\u662f\u5168\u8fde\u63a5\uff1b RNN # LSTM # Understanding LSTM Networks Bi-RNN # encoder-decoder/seq2seq # encoder-align model-decoder # paper Neural Machine Translation by Jointly Learning to Align and Translate \u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u67b6\u6784\uff0c\u5b83\u7684model topology\u5728blog Attention mechanism \u8fd9\u7ed9\u51fa\u4e86\uff0c\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u80fd\u591f\u66f4\u52a0\u6df1\u523b\u7406\u89e3\u5b83\u7684\u672c\u8d28\uff1b","title":"Model-And-layer-And-computation-And-computational-graph"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#_1","text":"","title":"\u524d\u8a00"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#model","text":"\u76ee\u524d\u5df2\u7ecf\u4f7f\u7528\u4e86\u4e09\u79cdmodel\uff1a MLP CNN RNN \u6bcf\u79cdmodel\u90fd\u8868\u793a\u4e00\u79cdcomputation\uff08\u8fd9\u662fmodel\u7684\u672c\u8d28\uff09\uff0c\u5728\u8bba\u6587\u4e2d\uff0c\u4e5f\u662f\u901a\u8fc7computation\uff08\u5404\u79cd\u5404\u6837\u7684\u516c\u5f0f\uff09\u6765\u63cf\u8ff0\u4e00\u4e2a\u6a21\u578b\u7684\uff1b\u800c\u5728\u5404\u79cdblog\u548ctutorial\u4e2d\uff0c\u5f80\u5f80\u559c\u6b22\u4f7f\u7528model\u7684topology\u6765\u4ecb\u7ecd\u5b83\uff1b\u663e\u7136model\u7684topology\u53ef\u4ee5\u770b\u505a\u662f\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u8868\u793a\uff0c\u5b83\u4eec\u80fd\u591f\u5e2e\u52a9\u5b66\u4e60\u8005\u66f4\u5feb\u5730\u5b66\u4e60model\uff0c\u5e2e\u52a9\u5b66\u4e60\u8005\u7406\u89e3model\u7684computation\uff1b \u5728deep learning book\u76846.5.1 Computational Graphs\u4e2d\u4ecb\u7ecd\uff0c\u6211\u4eec\u53ef\u4ee5computationaln graph\u6765formalizing computation as graph\u3002\u5728TensorFlow\u7684low API\u4e2d\u6307\u51fa\uff1aTensorFlow\u4e5f\u662f\u4f7f\u7528 computational graph \u6765\u5b9e\u73b0\u5e95\u5c42\u8ba1\u7b97\u7684\u63cf\u8ff0\u7684\uff08 A TensorFlow computation, represented as a dataflow graph. tensorflow\u4e2d\u662f\u4f7f\u7528\u7684dataflow graph\u6765\u8868\u793acomputation\u7684\uff09 \u8981\u60f3\u5b8c\u6574\u5730\u638c\u63e1\u4e00\u4e2a\u6a21\u578b\uff0c\u9700\u8981\u4ececomputation\uff0cmodel topology\uff0ccomputation graph\u8fd9\u4e09\u4e2a\u65b9\u9762\u5165\u624b\uff1b\u5176\u5b9ecomputation\u624d\u662f\u672c\u8d28\u6240\u5728\uff0cmodel topology\uff0ccomputation graph\u90fd\u662f\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff1b\u9664\u6b64\u4e4b\u5916\uff0c\u76ee\u524d\u5927\u591a\u6570deep learning framework\u7684\u5b9e\u73b0\u90fd\u662f\u91c7\u7528\u7684\u57fa\u4e8etensor\u7684\u6570\u636e\u8868\u793a\u3001\u57fa\u4e8etensor\u7684computation\uff0c\u4ece\u6574\u4f53\u6765\u770b\u8fd9\u4e2amodel\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a\u51fd\u6570 f(input_tensor)= outout_tensor \uff0c input_tensor \u6d41\u7ecfmodel\u5f97\u5230 output_tensor \uff0c\u6240\u4ee5\u638c\u63e1tensor\u5728model\u4e2d\u7684\u6d41\u52a8\u8fc7\u7a0b\uff08\u6d41\u52a8\u8fc7\u7a0b\u4e2dtensor\u7684shape\u7684\u53d8\u5316\u7b49\uff09\u4e5f\u662f\u638c\u63e1model\u7684computation\u7684\u4e00\u4e2a\u6377\u5f84\uff1b\u66f4\u52a0\u51c6\u786e\u5730\u8bf4\u4ed6\u4eec\u662f\u4f7f\u7528 dataflow graph \u6765\u8868\u793acomputation\uff0c\u6bd4\u5982tensorflow\uff1b \u5728\u6a21\u578b\u8bbe\u8ba1\u4e2d\u6240\u8bbe\u8ba1\u5730\u5404\u79cd\u601d\u60f3\uff0c\u5982parameter sharing\u7b49\uff0c\u90fd\u662f\u6709\u5bf9\u5e94\u7684computation\u7684\uff1b Neural Networks, Manifolds, and Topology","title":"model"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#layer","text":"\u6309\u7167\u76ee\u524d\u7684\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u5982tensorflow\uff0cKeras\uff0c\u90fd\u662f\u5c06model\u62bd\u8c61\u6210\u7531\u591a\u4e2alayer\u53e0\u52a0\u800c\u6210\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u662f\u5148\u4ece\u5b9e\u73b0\u5c42\u5165\u624b\uff1b\u5e76\u4e14\u5f88\u591a\u7684\u8bba\u6587\u4e2d\u4e5f\u662f\u8fd9\u6837\u63cf\u8ff0\u7684\uff1b\u4ecedeep learning book\u7684chapter 6\u4e2d\uff0c\u4e5f\u5c06\u8fd9\u4e2amodel\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u590d\u5408\u51fd\u6570\uff0c\u590d\u5408\u51fd\u6570\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5bf9\u5e94\u4e86\u4e00\u5c42\uff1b","title":"layer"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#computational-graph","text":"\u5176\u5b9ecomputation graph\u5e76\u6ca1\u6709\u4e25\u683c\u7684\u5b9a\u4e49\uff0c\u5728deep learning book\u4e2d\u7684computation graph\u5c31\u548cTensorFlow\u7684computation graph\u662f\u5b9a\u4e49\u5c31\u662f\u4e0d\u540c\u7684\uff1b deep learning book\u4e2d\u7684computation graph\u4fa7\u91cd\u70b9\u5728\u4e8e\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff0c\u5b83\u7684\u89c4\u5219\u5982\u4e0b\uff1a - node in the graph to indicate a variable TensorFlow\u7684 computational graph \u7684\u5b9a\u4e49\u5982\u4e0b\uff1a tf.Operation (or \"ops\"): The nodes of the graph. Operations describe calculations that consume and produce tensors. tf.Tensor : The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors . \u663e\u7136\uff0cTensorFlow\u4e2d\u7684\u5b9a\u4e49\u548cdeep learning book\u4e2d\u7684\u5b9a\u4e49\u662f\u4e0d\u540c\u7684\uff0c\u76f4\u89c2\u662f\u89c9\u5f97TensorFlow\u4e2d computational graph \u7684\u5b9a\u4e49\u662f\u6bd4\u8f83\u9002\u5408\u4e8e\u7f16\u7801\u5b9e\u73b0\u7684\uff0c\u800cdeep learning book\u4e2d\u7684\u5b9a\u4e49\u662f\u4fbf\u4e8e\u5bf9computation\u7684\u5f62\u5f0f\u5316\u5730\u5c55\u793a\uff08\u5f53\u7136TensorFlow\u7684computational graph\u4e5f\u80fd\u591f\u8fbe\u5230\u8fd9\u4e2a\u76ee\u7684\uff09\uff1b \u5728\u4e0b\u9762\u7684\u63cf\u8ff0\u4e2d\uff0ccomputational graph\u7684\u63cf\u8ff0\u90fd\u662f\u6309\u7167TensorFlow\u4e2d\u7684\u5b9a\u4e49\uff1b","title":"computational graph"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#mlp","text":"MLP\u7684model topology\uff1a full connected\uff08\u8f93\u5165\u5c42\u4e0e\u9690\u85cf\u5c42\u4e4b\u95f4\uff0c\u9690\u85cf\u5c42\u4e0e\u9690\u85cf\u5c42\u4e4b\u95f4\uff0c\u9690\u85cf\u5c42\u4e0e\u8f93\u51fa\u5c42\u4e4b\u95f4\uff0c\u90fd\u662f\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u8fde\u63a5\u7684\uff09 MLP\u7684computation\uff1a matrix multiplication \u4e0b\u9762\u7684\u4ee3\u7801\u662f\u6458\u81ea Getting started with the Keras Sequential model \u7684Multilayer Perceptron (MLP) for multi-class softmax classification: import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u601d\u8003\uff1a\u4e0a\u8ff0\u4ee3\u7801\u6240\u6784\u5efa\u7684\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u662f\u4ec0\u4e48\uff1f \u7b54\uff1a\u4e00\u822c \u8f93\u5165\u77e9\u9635 \u7684shape\u662f [batch_size, feature_num] \uff0c\u5219\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f [feature_num, hidden_layer_node_num_1] \uff0c\u5373\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\u548c batch_size \u65e0\u5173\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u7279\u6027\uff1a\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u4e0d\u77e5\u9053 batch_size \u7684\u60c5\u51b5\u4e0b\u5c31\u53ef\u4ee5\u6784\u5efa\u6a21\u578b\u4e86\uff0c\u65e0\u8bba\u5bf9\u4e8e\u4ec0\u4e48\u6a21\u578b\uff0c\u6a21\u578b\u4e2d\u6bcf\u4e00\u5c42\u7684 \u8282\u70b9\u6570 \u548c \u7279\u5f81\u7684\u4e2a\u6570 \u3001 batch_size \u90fd\u662f\u6ca1\u6709\u5173\u8054\u7684\uff08\u5f53\u6d89\u53caLSTM\u7684\u65f6\u5019\uff0cLSTM\u4e2d\u6bcf\u4e00\u5c42\u7684neuron\u7684\u4e2a\u6570\u548c time_step \u4e5f\u662f\u6ca1\u6709\u5173\u8054\u7684\uff09\uff0c \u7279\u5f81\u7684\u4e2a\u6570 \u4f1a\u5f71\u54cdneuron\u4e2d\u7684\u53c2\u6570\u4e2a\u6570\u6709\u5173\uff1b \u7531\u4e8eMLP\u8981\u6c42full connected\uff0c\u4f7f\u7528 \u77e9\u9635\u4e58\u6cd5 \u662f\u80fd\u591f\u975e\u5e38\u597d\u5730\u5b9e\u73b0\u8fd9\u79cd\u9700\u6c42\u7684\uff0c\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff1a [ [1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1], ] 4*3 \u7b2c\u4e00\u9690\u85cf\u5c42\u670910\u4e2anode\uff0c\u5219\u5b83\u7684\u6743\u91cd\u77e9\u9635\u662f[3 * 10] [ [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], ] \u6bcf\u4e00\u5217\u8868\u793a\u7684\u662f \u7b2c\u4e8c\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\uff1a [hidden_layer_node_num_1, hidden_layer_node_num_2] \uff0c\u4f9d\u6b21\u7c7b\u63a8\uff0c\u6240\u4ee5\u6700\u7ec8\u6700\u540e\u4e00\u5c42\u5373\u8f93\u51fa\u5c42\u7684\u4e0e\u524d\u4e00\u5c42\u4e4b\u95f4\u7684\u6743\u91cd\u77e9\u9635 [hidden_layer_node_num_-1, n_class] \uff08 -1 \u8868\u793a\u6700\u540e\u4e00\u5c42\uff09\u3002 \u6240\u4ee5\uff0c\u4e00\u4e2abatch_size\u7684\u6570\u636e\u6d41\u7ecfMLP\u4e4b\u540e\uff0c\u6700\u7ec8\u5f97\u5230\u7684\u6570\u636e\u7684shape\u662f [batch_size, n_classes] \u3002 \u5176\u5b9e\u4ece\u8fd9\u4e2a\u6570\u5b66\u5173\u7cfb\u4e5f\u53ef\u4ee5\u770b\u51fa\u4e3a\u4ec0\u4e48\u8981\u5c06label\u4ee5one-hot\u7684\u65b9\u5f0f\u8868\u793a\u4e86\uff1b \u4e0b\u9762\u7684\u4ee3\u7801\u662fTensorFlow\u4e2d\u6784\u5efaMLP\u7684\u4e00\u4e2ademo\uff1a with tf.name_scope('input'): self.x_in = tf.placeholder(tf.float32, [None, self.feature_num], name='x_in') # self.y_in = tf.placeholder(tf.float32, [None, self.time_step, self.n_classes], name='y_in') # \u6bcf\u4e2asequence\u90fd\u53d6\u6700\u540e\u4e00\u6761\u8bb0\u5f55\u7684target\u6765\u4f5c\u4e3a\u8fd9\u4e2asequence\u7684target self.y_in = tf.placeholder(tf.float32, [None, self.n_classes], name='y_in') self.keep_prob = tf.placeholder(tf.float32, name='dropout_in') with tf.name_scope('layer1'): w_in = self.__weight_variable__([self.feature_num, self.num_lstm_units]) b_in = self.__bias_variable__([self.num_lstm_units]) lstm_input_layer = tf.reshape(self.x_in, [-1, self.feature_num]) # input layer lstm_input_layer = tf.nn.relu(tf.matmul(lstm_input_layer, w_in) + b_in) \u4e00\u822c\uff0c\u6211\u4eec\u5728\u9605\u8bfb\u4e66\u7c4d\u7684\u65f6\u5019\uff0c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u6d41\u7a0b\u90fd\u662f\u4e00\u6b21\u8f93\u5165\u4e00\u6761\u8bb0\u5f55\uff0c\u8fd9\u79cd\u505a\u6cd5\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5b9e\u9645\u4e0a\u5982\u679c\u771f\u6ef4\u4e00\u6b21\u4ec5\u4ec5\u5582\u5165\u4e00\u6761\u6570\u636e\u7684\u8bdd\uff0c\u4f1a\u975e\u5e38\u7f13\u6162\uff1b\u5b9e\u9645\u7684\u5b9e\u73b0\u662f\u4e00\u6b21\u5582\u5165\u4e00\u4e2abatch\u7684\uff0c\u5373\u662f\u4e0a\u9762\u6240\u63cf\u8ff0\u7684 \u8f93\u5165\u77e9\u9635 \uff0c\u73b0\u4ee3\u7684GPU\u5904\u7406\u77e9\u9635\u8fd0\u7b97\u7684\u901f\u5ea6\u975e\u5e38\u5feb\uff1b\u5176\u5b9e\u4e00\u6b21\u5582\u5165\u4e00\u6761\u8bb0\u5f55\u4e5f\u53ef\u4ee5\u5957\u7528\u4e0a\u9762\u7684\u77e9\u9635\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5373 batch_size=1 \uff1b \u4eceTensorFlow\u7684\u4ee3\u7801\u53ef\u4ee5\u770b\u51fa\uff0c\u8f93\u5165\u77e9\u9635\u548c\u7b2c\u4e00\u5c42\u7684\u6743\u91cd\u77e9\u9635\u6267\u884c\u77e9\u9635\u4e58\u6cd5\uff0c\u6839\u636e\u77e9\u9635 \u4e58\u6cd5\u539f\u7406 \u53ef\u4ee5\u77e5\u9053\u6bcf\u4e00\u6761\u6570\u636e\u4f1a\u6d41\u5165\u5230\u7b2c\u4e00\u9690\u85cf\u5c42\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u6761\u8bb0\u5f55\u6d41\u5165\u4e00\u4e2a\u8282\u70b9\u4ea7\u751f\u7684\u8f93\u51fa\u5176\u5b9e\u662f\u4e00\u4e2a\u6807\u91cf\uff1b\u5176\u5b9e\u8fd9\u4e5f\u662ffull connected\u7684\u542b\u4e49\u6240\u5728\uff1b","title":"MLP"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#cnn","text":"\u5173\u4e8eCNN\u7684computation\u548cmodel topology\u53c2\u8003\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u975e\u5e38\u597d\u7684\u89e3\u91ca\uff1a Convolutional Neural Networks (CNNs / ConvNets) A Beginner's Guide To Understanding Convolutional Neural Networks \u5168\u8fde\u63a5\u5bf9\u5e94\u7684\u662f\u77e9\u9635\u4e58\u6cd5\uff0cCNN\u4e2d\u7684filter\u5219\u5bf9\u5e94\u7684\u5377\u79ef\u8fd0\u7b97\uff0c\u5377\u79ef\u5c42\u4e2d\u7684\u795e\u7ecf\u5143\u53ea\u4f1a\u548cinput\u7684\u4e00\u90e8\u5206\u8fdb\u884c\u8fde\u63a5\uff0c\u800c\u4e0d\u662f\u5168\u8fde\u63a5\uff1b","title":"CNN"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#rnn","text":"","title":"RNN"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#lstm","text":"Understanding LSTM Networks","title":"LSTM"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#bi-rnn","text":"","title":"Bi-RNN"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#encoder-decoderseq2seq","text":"","title":"encoder-decoder/seq2seq"},{"location":"Programming/Model-And-layer-And-computation-And-computational-graph/#encoder-align-model-decoder","text":"paper Neural Machine Translation by Jointly Learning to Align and Translate \u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u67b6\u6784\uff0c\u5b83\u7684model topology\u5728blog Attention mechanism \u8fd9\u7ed9\u51fa\u4e86\uff0c\u4e24\u8005\u7ed3\u5408\u8d77\u6765\u80fd\u591f\u66f4\u52a0\u6df1\u523b\u7406\u89e3\u5b83\u7684\u672c\u8d28\uff1b","title":"encoder-align model-decoder"},{"location":"Programming/Programming-paradigm-of-deep-learning/","text":"rule one\uff1aseparation of model and dataset use trainer to connect dataset and model feedforward and feedback rule one\uff1aseparation of model and dataset # \u5c06model\u548cdataset\u5206\u9694\u5f00\u6765 use trainer to connect dataset and model # feedforward and feedback # \u524d\u9988\u8fc7\u7a0b\uff1a\u8ba1\u7b97\u5f97\u5230loss \u53cd\u9988\u8fc7\u7a0b\uff1abackpropagation to compute gradient and then adjust parameter\uff1b","title":"Programming-paradigm-of-deep-learning"},{"location":"Programming/Programming-paradigm-of-deep-learning/#rule-oneseparation-of-model-and-dataset","text":"\u5c06model\u548cdataset\u5206\u9694\u5f00\u6765","title":"rule one\uff1aseparation of model and dataset"},{"location":"Programming/Programming-paradigm-of-deep-learning/#use-trainer-to-connect-dataset-and-model","text":"","title":"use trainer to connect dataset and model"},{"location":"Programming/Programming-paradigm-of-deep-learning/#feedforward-and-feedback","text":"\u524d\u9988\u8fc7\u7a0b\uff1a\u8ba1\u7b97\u5f97\u5230loss \u53cd\u9988\u8fc7\u7a0b\uff1abackpropagation to compute gradient and then adjust parameter\uff1b","title":"feedforward and feedback"},{"location":"Programming/VS-pytorch-vs-tensorflow/","text":"https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b","title":"VS-pytorch-vs-tensorflow"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/","text":"Getting started with the Keras Sequential model Specifying the input shape Compilation Training Examples Multilayer Perceptron (MLP) for multi-class softmax classification: MLP for binary classification: VGG-like convnet: Sequence classification with LSTM: Sequence classification with 1D convolutions: Stacked LSTM for sequence classification Same stacked LSTM model, rendered \"stateful\" Getting started with the Keras Sequential model # The Sequential model is a linear stack of layers. You can create a Sequential model by passing a list of layer instances to the constructor: from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax'), ]) You can also simply add layers via the .add() method: model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu')) Specifying the input shape # The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference ) needs to receive information about its input shape . There are several possible ways to do this: Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape , the batch dimension is not included. Some 2D layers, such as Dense , support the specification of their input shape via the argument input_dim , and some 3D temporal layers support the arguments input_dim and input_length . If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks ), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8) . As such, the following snippets are strictly equivalent: model = Sequential() model.add(Dense(32, input_shape=(784,))) model = Sequential() model.add(Dense(32, input_dim=784)) Compilation # Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments: An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad ), or an instance of the Optimizer class. See: optimizers . A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse ), or it can be an objective function. See: losses . A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function. # For a multi-class classification problem model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # For a binary classification problem model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # For a mean squared error regression problem model.compile(optimizer='rmsprop', loss='mse') # For custom metrics import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) Training # Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. Read its documentation here . # For a single-input model with 2 classes (binary classification): model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # Train the model, iterating on the data in batches of 32 samples model.fit(data, labels, epochs=10, batch_size=32) # For a single-input model with 10 classes (categorical classification): model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(10, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # Convert labels to categorical one-hot encoding one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.fit(data, one_hot_labels, epochs=10, batch_size=32) Examples # Here are a few examples to get you started! In the examples folder , you will also find example models for real datasets: CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation IMDB movie review sentiment classification: LSTM over sequences of words Reuters newswires topic classification: Multilayer Perceptron (MLP) MNIST handwritten digits classification: MLP & CNN Character-level text generation with LSTM ...and more. Multilayer Perceptron (MLP) for multi-class softmax classification: # import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u601d\u8003\uff1a\u4e0a\u8ff0\u6743\u91cd\u77e9\u9635\u662f\u4ec0\u4e48\uff1f\u4e00\u822c\u8f93\u5165\u77e9\u9635\u7684shape\u662f [batch_size, feature_num] \uff0c\u5219\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f [feature_num, hidden_layer_node_num_1] \uff1b \u663e\u7136\uff0c\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\u548c batch_size \u65e0\u5173\u7684\uff1b \u4e00\u822c\uff0c\u6211\u4eec\u5728\u9605\u8bfb\u4e66\u7c4d\u7684\u65f6\u5019\uff0c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u6d41\u7a0b\u90fd\u662f\u4e00\u6b21\u8f93\u5165\u4e00\u6761\u8bb0\u5f55\uff0c\u8fd9\u79cd\u505a\u6cd5\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5b9e\u9645\u4e0a\u5982\u679c\u771f\u6ef4\u4e00\u6b21\u4ec5\u4ec5\u5582\u5165\u4e00\u6761\u6570\u636e\u7684\u8bdd\uff0c\u4f1a\u975e\u5e38\u7f13\u6162\uff1b\u5b9e\u9645\u7684\u5b9e\u73b0\u662f\u4e00\u6b21\u5582\u5165\u4e00\u4e2abatch\u7684\uff0c\u5373\u662f\u4e0a\u9762\u6240\u63cf\u8ff0\u7684\u8f93\u5165\u77e9\u9635\uff0c\u73b0\u4ee3\u7684GPU\u5904\u7406\u77e9\u9635\u8fd0\u7b97\u7684\u901f\u5ea6\u975e\u5e38\u5feb\uff1b\u5176\u5b9e\u4e00\u6b21\u5582\u5165\u4e00\u6761\u8bb0\u5f55\u4e5f\u53ef\u4ee5\u5957\u7528\u4e0a\u9762\u7684\u77e9\u9635\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5373 batch_size=1 \uff1b\u6839\u636e\u77e9\u9635\u7684\u4e58\u6cd5\u539f\u7406\u53ef\u4ee5\u4ece\u77e5\u9053\u6bcf\u4e00\u6761\u6570\u636e\u4f1a\u6d41\u5165\u5230\u7b2c\u4e00\u9690\u85cf\u5c42\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u6761\u8bb0\u5f55\u6d41\u5165\u4e00\u4e2a\u8282\u70b9\u4ea7\u751f\u7684\u8f93\u51fa\u5176\u5b9e\u662f\u4e00\u4e2a\u6807\u91cf\uff1b \u65e0\u8bba\u5bf9\u4e8e\u4ec0\u4e48\u6a21\u578b\uff0c\u4e0a\u8ff0\u539f\u7406\u90fd\u662f\u901a\u7528\u7684\uff1b\u6240\u4ee5\uff0c\u6a21\u578b\u4e2d\u6bcf\u4e00\u5c42\u7684\u8282\u70b9\u6570\u548c\u7279\u5f81\u7684\u4e2a\u6570\u662f\u6ca1\u6709\u5173\u8054\u7684\uff1b\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b [ [1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1], ] 4*3 \u7b2c\u4e00\u9690\u85cf\u5c42\u670910\u4e2anode\uff0c\u5219\u5b83\u7684\u6743\u91cd\u77e9\u9635\u662f[3 * 10] [ [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], ] \u6bcf\u4e00\u5217\u8868\u793a\u7684\u662f \u7b2c\u4e8c\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\uff1a [hidden_layer_node_num_1, hidden_layer_node_num_2] \uff0c\u4f9d\u6b21\u7c7b\u63a8\uff0c\u6240\u4ee5\u6700\u7ec8\u6700\u540e\u4e00\u5c42\u5373\u8f93\u51fa\u5c42\u7684\u4e0e\u524d\u4e00\u5c42\u4e4b\u95f4\u7684\u6743\u91cd\u77e9\u9635 [hidden_layer_node_num_-1, n_class] \uff08 -1 \u8868\u793a\u6700\u540e\u4e00\u5c42\uff09\u3002 \u6240\u4ee5\uff0c\u6700\u7ec8\u4e00\u4e2abatch_size\u7684\u6570\u636e\u6d41\u7ecfMLP\u4e4b\u540e\uff0c\u5f97\u5230\u7684\u6570\u636e\u7684shape\u662f [batch_size, n_classes] \u3002 \u5176\u5b9e\u4ece\u8fd9\u4e2a\u6570\u5b66\u5173\u7cfb\u4e5f\u53ef\u4ee5\u770b\u51fa\u4e3a\u4ec0\u4e48\u8981\u5c06label\u4ee5one-hot\u7684\u65b9\u5f0f\u8868\u793a\u4e86\uff1b MLP for binary classification: # import numpy as np from keras.models import Sequential from keras.layers import Dense, Dropout # Generate dummy data x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) VGG-like convnet: # import numpy as np import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD # Generate dummy data x_train = np.random.random((100, 100, 100, 3)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Sequential() # input: 100x100 images with 3 channels -> (100, 100, 3) tensors. # this applies 32 convolution filters of size 3x3 each. model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) model.fit(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32) Sequence classification with LSTM: # from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import LSTM max_features = 1024 model = Sequential() model.add(Embedding(max_features, output_dim=256)) model.add(LSTM(128)) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) Sequence classification with 1D convolutions: # from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D seq_length = 64 model = Sequential() model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Conv1D(64, 3, activation='relu')) model.add(MaxPooling1D(3)) model.add(Conv1D(128, 3, activation='relu')) model.add(Conv1D(128, 3, activation='relu')) model.add(GlobalAveragePooling1D()) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16) Stacked LSTM for sequence classification # In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations. The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence , thus dropping the temporal dimension (i.e. converting the input sequence into a single vector). from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # expected input data shape: (batch_size, timesteps, data_dim) model = Sequential() model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 32 model.add(LSTM(32, return_sequences=True)) # returns a sequence of vectors of dimension 32 model.add(LSTM(32)) # return a single vector of dimension 32 model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # Generate dummy validation data x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val)) Same stacked LSTM model, rendered \"stateful\" # A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable. You can read more about stateful RNNs in the FAQ. from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # Expected input batch shape: (batch_size, timesteps, data_dim) # Note that we have to provide the full batch_input_shape since the network is stateful. # the sample of index i in batch k is the follow-up for the sample i in batch k-1. model = Sequential() model.add(LSTM(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(LSTM(32, return_sequences=True, stateful=True)) model.add(LSTM(32, stateful=True)) model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # Generate dummy validation data x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.fit(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"Getting started with the Keras Sequential model"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#getting-started-with-the-keras-sequential-model","text":"The Sequential model is a linear stack of layers. You can create a Sequential model by passing a list of layer instances to the constructor: from keras.models import Sequential from keras.layers import Dense, Activation model = Sequential([ Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax'), ]) You can also simply add layers via the .add() method: model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu'))","title":"Getting started with the Keras Sequential model"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#specifying-the-input-shape","text":"The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference ) needs to receive information about its input shape . There are several possible ways to do this: Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape , the batch dimension is not included. Some 2D layers, such as Dense , support the specification of their input shape via the argument input_dim , and some 3D temporal layers support the arguments input_dim and input_length . If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks ), you can pass a batch_size argument to a layer. If you pass both batch_size=32 and input_shape=(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8) . As such, the following snippets are strictly equivalent: model = Sequential() model.add(Dense(32, input_shape=(784,))) model = Sequential() model.add(Dense(32, input_dim=784))","title":"Specifying the input shape"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#compilation","text":"Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments: An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad ), or an instance of the Optimizer class. See: optimizers . A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse ), or it can be an objective function. See: losses . A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function. # For a multi-class classification problem model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # For a binary classification problem model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # For a mean squared error regression problem model.compile(optimizer='rmsprop', loss='mse') # For custom metrics import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])","title":"Compilation"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#training","text":"Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. Read its documentation here . # For a single-input model with 2 classes (binary classification): model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # Train the model, iterating on the data in batches of 32 samples model.fit(data, labels, epochs=10, batch_size=32) # For a single-input model with 10 classes (categorical classification): model = Sequential() model.add(Dense(32, activation='relu', input_dim=100)) model.add(Dense(10, activation='softmax')) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # Convert labels to categorical one-hot encoding one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.fit(data, one_hot_labels, epochs=10, batch_size=32)","title":"Training"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#examples","text":"Here are a few examples to get you started! In the examples folder , you will also find example models for real datasets: CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation IMDB movie review sentiment classification: LSTM over sequences of words Reuters newswires topic classification: Multilayer Perceptron (MLP) MNIST handwritten digits classification: MLP & CNN Character-level text generation with LSTM ...and more.","title":"Examples"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#multilayer-perceptron-mlp-for-multi-class-softmax-classification","text":"import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.optimizers import SGD # Generate dummy data import numpy as np x_train = np.random.random((1000, 20)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) x_test = np.random.random((100, 20)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) model = Sequential() # Dense(64) is a fully-connected layer with 64 hidden units. # in the first layer, you must specify the expected input data shape: # here, 20-dimensional vectors. model.add(Dense(64, activation='relu', input_dim=20)) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128) \u601d\u8003\uff1a\u4e0a\u8ff0\u6743\u91cd\u77e9\u9635\u662f\u4ec0\u4e48\uff1f\u4e00\u822c\u8f93\u5165\u77e9\u9635\u7684shape\u662f [batch_size, feature_num] \uff0c\u5219\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f [feature_num, hidden_layer_node_num_1] \uff1b \u663e\u7136\uff0c\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\u548c batch_size \u65e0\u5173\u7684\uff1b \u4e00\u822c\uff0c\u6211\u4eec\u5728\u9605\u8bfb\u4e66\u7c4d\u7684\u65f6\u5019\uff0c\u4e66\u4e2d\u6240\u63cf\u8ff0\u7684\u6d41\u7a0b\u90fd\u662f\u4e00\u6b21\u8f93\u5165\u4e00\u6761\u8bb0\u5f55\uff0c\u8fd9\u79cd\u505a\u6cd5\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5b9e\u9645\u4e0a\u5982\u679c\u771f\u6ef4\u4e00\u6b21\u4ec5\u4ec5\u5582\u5165\u4e00\u6761\u6570\u636e\u7684\u8bdd\uff0c\u4f1a\u975e\u5e38\u7f13\u6162\uff1b\u5b9e\u9645\u7684\u5b9e\u73b0\u662f\u4e00\u6b21\u5582\u5165\u4e00\u4e2abatch\u7684\uff0c\u5373\u662f\u4e0a\u9762\u6240\u63cf\u8ff0\u7684\u8f93\u5165\u77e9\u9635\uff0c\u73b0\u4ee3\u7684GPU\u5904\u7406\u77e9\u9635\u8fd0\u7b97\u7684\u901f\u5ea6\u975e\u5e38\u5feb\uff1b\u5176\u5b9e\u4e00\u6b21\u5582\u5165\u4e00\u6761\u8bb0\u5f55\u4e5f\u53ef\u4ee5\u5957\u7528\u4e0a\u9762\u7684\u77e9\u9635\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5373 batch_size=1 \uff1b\u6839\u636e\u77e9\u9635\u7684\u4e58\u6cd5\u539f\u7406\u53ef\u4ee5\u4ece\u77e5\u9053\u6bcf\u4e00\u6761\u6570\u636e\u4f1a\u6d41\u5165\u5230\u7b2c\u4e00\u9690\u85cf\u5c42\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8282\u70b9\uff0c\u4e00\u6761\u8bb0\u5f55\u6d41\u5165\u4e00\u4e2a\u8282\u70b9\u4ea7\u751f\u7684\u8f93\u51fa\u5176\u5b9e\u662f\u4e00\u4e2a\u6807\u91cf\uff1b \u65e0\u8bba\u5bf9\u4e8e\u4ec0\u4e48\u6a21\u578b\uff0c\u4e0a\u8ff0\u539f\u7406\u90fd\u662f\u901a\u7528\u7684\uff1b\u6240\u4ee5\uff0c\u6a21\u578b\u4e2d\u6bcf\u4e00\u5c42\u7684\u8282\u70b9\u6570\u548c\u7279\u5f81\u7684\u4e2a\u6570\u662f\u6ca1\u6709\u5173\u8054\u7684\uff1b\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b [ [1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1], ] 4*3 \u7b2c\u4e00\u9690\u85cf\u5c42\u670910\u4e2anode\uff0c\u5219\u5b83\u7684\u6743\u91cd\u77e9\u9635\u662f[3 * 10] [ [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], [2,2,2,2,2,2,2,2,2,2], ] \u6bcf\u4e00\u5217\u8868\u793a\u7684\u662f \u7b2c\u4e8c\u9690\u85cf\u5c42\u7684\u6743\u91cd\u77e9\u9635\u7684shape\u662f\uff1a [hidden_layer_node_num_1, hidden_layer_node_num_2] \uff0c\u4f9d\u6b21\u7c7b\u63a8\uff0c\u6240\u4ee5\u6700\u7ec8\u6700\u540e\u4e00\u5c42\u5373\u8f93\u51fa\u5c42\u7684\u4e0e\u524d\u4e00\u5c42\u4e4b\u95f4\u7684\u6743\u91cd\u77e9\u9635 [hidden_layer_node_num_-1, n_class] \uff08 -1 \u8868\u793a\u6700\u540e\u4e00\u5c42\uff09\u3002 \u6240\u4ee5\uff0c\u6700\u7ec8\u4e00\u4e2abatch_size\u7684\u6570\u636e\u6d41\u7ecfMLP\u4e4b\u540e\uff0c\u5f97\u5230\u7684\u6570\u636e\u7684shape\u662f [batch_size, n_classes] \u3002 \u5176\u5b9e\u4ece\u8fd9\u4e2a\u6570\u5b66\u5173\u7cfb\u4e5f\u53ef\u4ee5\u770b\u51fa\u4e3a\u4ec0\u4e48\u8981\u5c06label\u4ee5one-hot\u7684\u65b9\u5f0f\u8868\u793a\u4e86\uff1b","title":"Multilayer Perceptron (MLP) for multi-class softmax classification:"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#mlp-for-binary-classification","text":"import numpy as np from keras.models import Sequential from keras.layers import Dense, Dropout # Generate dummy data x_train = np.random.random((1000, 20)) y_train = np.random.randint(2, size=(1000, 1)) x_test = np.random.random((100, 20)) y_test = np.random.randint(2, size=(100, 1)) model = Sequential() model.add(Dense(64, input_dim=20, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, epochs=20, batch_size=128) score = model.evaluate(x_test, y_test, batch_size=128)","title":"MLP for binary classification:"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#vgg-like-convnet","text":"import numpy as np import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import SGD # Generate dummy data x_train = np.random.random((100, 100, 100, 3)) y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) x_test = np.random.random((20, 100, 100, 3)) y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10) model = Sequential() # input: 100x100 images with 3 channels -> (100, 100, 3) tensors. # this applies 32 convolution filters of size 3x3 each. model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='categorical_crossentropy', optimizer=sgd) model.fit(x_train, y_train, batch_size=32, epochs=10) score = model.evaluate(x_test, y_test, batch_size=32)","title":"VGG-like convnet:"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#sequence-classification-with-lstm","text":"from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import LSTM max_features = 1024 model = Sequential() model.add(Embedding(max_features, output_dim=256)) model.add(LSTM(128)) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"Sequence classification with LSTM:"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#sequence-classification-with-1d-convolutions","text":"from keras.models import Sequential from keras.layers import Dense, Dropout from keras.layers import Embedding from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D seq_length = 64 model = Sequential() model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100))) model.add(Conv1D(64, 3, activation='relu')) model.add(MaxPooling1D(3)) model.add(Conv1D(128, 3, activation='relu')) model.add(Conv1D(128, 3, activation='relu')) model.add(GlobalAveragePooling1D()) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=16, epochs=10) score = model.evaluate(x_test, y_test, batch_size=16)","title":"Sequence classification with 1D convolutions:"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#stacked-lstm-for-sequence-classification","text":"In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations. The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence , thus dropping the temporal dimension (i.e. converting the input sequence into a single vector). from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 # expected input data shape: (batch_size, timesteps, data_dim) model = Sequential() model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim))) # returns a sequence of vectors of dimension 32 model.add(LSTM(32, return_sequences=True)) # returns a sequence of vectors of dimension 32 model.add(LSTM(32)) # return a single vector of dimension 32 model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((1000, timesteps, data_dim)) y_train = np.random.random((1000, num_classes)) # Generate dummy validation data x_val = np.random.random((100, timesteps, data_dim)) y_val = np.random.random((100, num_classes)) model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))","title":"Stacked LSTM for sequence classification"},{"location":"Programming/software-Keras/Getting-started-with-the-Keras-Sequential-model/#same-stacked-lstm-model-rendered-stateful","text":"A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable. You can read more about stateful RNNs in the FAQ. from keras.models import Sequential from keras.layers import LSTM, Dense import numpy as np data_dim = 16 timesteps = 8 num_classes = 10 batch_size = 32 # Expected input batch shape: (batch_size, timesteps, data_dim) # Note that we have to provide the full batch_input_shape since the network is stateful. # the sample of index i in batch k is the follow-up for the sample i in batch k-1. model = Sequential() model.add(LSTM(32, return_sequences=True, stateful=True, batch_input_shape=(batch_size, timesteps, data_dim))) model.add(LSTM(32, return_sequences=True, stateful=True)) model.add(LSTM(32, stateful=True)) model.add(Dense(10, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # Generate dummy training data x_train = np.random.random((batch_size * 10, timesteps, data_dim)) y_train = np.random.random((batch_size * 10, num_classes)) # Generate dummy validation data x_val = np.random.random((batch_size * 3, timesteps, data_dim)) y_val = np.random.random((batch_size * 3, num_classes)) model.fit(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=False, validation_data=(x_val, y_val))","title":"Same stacked LSTM model, rendered \"stateful\""},{"location":"Programming/software-Keras/intro-Keras/","text":"Keras: The Python Deep Learning library You have just found Keras. Guiding principles Getting started: 30 seconds to Keras Installation Configuring your Keras backend Keras: The Python Deep Learning library # You have just found Keras. # Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU. Read the documentation at Keras.io . Keras is compatible with: Python 2.7-3.6 . Guiding principles # User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive\uff08\u8ba4\u77e5\uff09 load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error. Modularity. A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers , cost functions , optimizers , initialization schemes , activation functions and regularization schemes are all standalone modules that you can combine to create new models. Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research. Work with Python . No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility. SUMMARY : \u597d\u7684software\u5c31\u5e94\u5f53\u63d0\u4f9b\u5982Keras\u8fd9\u6837\u7684API\uff1b Getting started: 30 seconds to Keras # The core data structure of Keras is a model , a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API , which allows to build arbitrary graphs of layers. SUMMARY : \u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\uff0cmodel\uff0cnetwork\uff0cgraph\u53ef\u4ee5\u770b\u505a\u662f\u540c\u4e49\u8bcd\uff1b Here is the Sequential model: from keras.models import Sequential model = Sequential() Stacking\uff08\u5806\uff09 layers is as easy as .add() : from keras.layers import Dense model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) Once your model looks good, configure its learning process with .compile() : model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code). model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) You can now iterate on your training data in batches: # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API. model.fit(x_train, y_train, epochs=5, batch_size=32) Alternatively, you can feed batches to your model manually: model.train_on_batch(x_batch, y_batch) Evaluate your performance in one line: loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) Or generate predictions on new data: classes = model.predict(x_test, batch_size=128) Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful? For a more in-depth tutorial about Keras, you can check out: Getting started with the Sequential model Getting started with the functional API In the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc. Installation # Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend. TensorFlow installation instructions . Theano installation instructions . CNTK installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install Keras itself. There are two ways to install Keras: Configuring your Keras backend # By default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.","title":"intro Keras"},{"location":"Programming/software-Keras/intro-Keras/#keras-the-python-deep-learning-library","text":"","title":"Keras: The Python Deep Learning library"},{"location":"Programming/software-Keras/intro-Keras/#you-have-just-found-keras","text":"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK , or Theano . It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU. Read the documentation at Keras.io . Keras is compatible with: Python 2.7-3.6 .","title":"You have just found Keras."},{"location":"Programming/software-Keras/intro-Keras/#guiding-principles","text":"User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive\uff08\u8ba4\u77e5\uff09 load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error. Modularity. A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers , cost functions , optimizers , initialization schemes , activation functions and regularization schemes are all standalone modules that you can combine to create new models. Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research. Work with Python . No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility. SUMMARY : \u597d\u7684software\u5c31\u5e94\u5f53\u63d0\u4f9b\u5982Keras\u8fd9\u6837\u7684API\uff1b","title":"Guiding principles"},{"location":"Programming/software-Keras/intro-Keras/#getting-started-30-seconds-to-keras","text":"The core data structure of Keras is a model , a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API , which allows to build arbitrary graphs of layers. SUMMARY : \u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\uff0cmodel\uff0cnetwork\uff0cgraph\u53ef\u4ee5\u770b\u505a\u662f\u540c\u4e49\u8bcd\uff1b Here is the Sequential model: from keras.models import Sequential model = Sequential() Stacking\uff08\u5806\uff09 layers is as easy as .add() : from keras.layers import Dense model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) Once your model looks good, configure its learning process with .compile() : model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code). model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) You can now iterate on your training data in batches: # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API. model.fit(x_train, y_train, epochs=5, batch_size=32) Alternatively, you can feed batches to your model manually: model.train_on_batch(x_batch, y_batch) Evaluate your performance in one line: loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) Or generate predictions on new data: classes = model.predict(x_test, batch_size=128) Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful? For a more in-depth tutorial about Keras, you can check out: Getting started with the Sequential model Getting started with the functional API In the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.","title":"Getting started: 30 seconds to Keras"},{"location":"Programming/software-Keras/intro-Keras/#installation","text":"Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend. TensorFlow installation instructions . Theano installation instructions . CNTK installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install Keras itself. There are two ways to install Keras:","title":"Installation"},{"location":"Programming/software-Keras/intro-Keras/#configuring-your-keras-backend","text":"By default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.","title":"Configuring your Keras backend"},{"location":"Programming/software-Keras/keras-Layers-Convolutional-Layers/","text":"Convolutional Layers Conv1D Input shape Output shape Convolutional Layers # Conv1D # keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D convolution layer (e.g. temporal convolution ). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\" , or (None, 128) for variable-length sequences with 128 features per step. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . SUMMARY : \u5173\u4e8e filters \u548c kernel_size \uff0c\u53c2\u89c1\u4e0b\u9762\u8fd9\u7bc7\u6587\u7ae0\uff1a Keras conv1d layer parameters: filters and kernel_size SUMMARY : \u5173\u4e8echannel-last\u548cchannel-first\uff0c\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0\uff1a A Gentle Introduction to Channels-First and Channels-Last Image Formats Input shape # 3D tensor with shape: (batch, steps, channels) SUMMARY : \u8981\u60f3\u7406\u89e3\u8fd9\u6bb5\u8bdd\u4e2d steps \u3001 channels \u7684\u542b\u4e49\uff0c\u9996\u5148\u9700\u8981\u4ed4\u7ec6\u9605\u8bfb\u4e0a\u9762\u7684\u7b2c\u4e09\u6bb5\uff0c\u5176\u4e2d\u5df2\u7ecf\u7ed9\u51fa\u4e86\u4e00\u4e2aexample\uff1b\u8fd9\u91cc\u6211\u518d\u8865\u5145\u4e00\u4e2a\u4f8b\u5b50\uff1a \u5982\u679c\u4ee5 Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks \u4e2d\u7684sentence\u4e3a\u4f8b\uff0c\u90a3\u4e48 steps \u5219\u8868\u793a\u7684sentence\u7684\u957f\u5ea6\uff0c\u5373sentence\u4e2dword\u7684\u4e2a\u6570\uff1b channels \u5219\u8868\u793aword embedding+position embedding\u7684\u957f\u5ea6\uff1b Output shape # 3D tensor with shape: (batch, new_steps, filters) steps value might have changed due to padding or strides. SUMMARY : \u4e0a\u8ff0output shape\u548c Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks \u4e2d\u63cf\u8ff0\u7684\u4e0d\u540c\uff1b SUMMARY : \u4e00\u822c\u5728\u8bb2\u89e3model\u7684\u539f\u7406\u65f6\u5019\u90fd\u662f\u4e0d\u4f1a\u6d89\u53ca\u5230 batch_size \u7684\uff0c\u800c\u662f\u4ec5\u4ec5\u4e00\u4e00\u6761\u8f93\u5165\u6570\u636e\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u4f46\u662f\u5b9e\u73b0\u5e93\u4e2d\u5219\u5fc5\u987b\u8981\u6d89\u53ca\u5230 batch_size \uff0c\u8fd9\u91cc\u4fbf\u662f\u8fd9\u6837\uff1b\u5176\u5b9e\u6211\u89c9\u5f97\u5e94\u8be5\u8fd9\u6837\u6765\u7406\u89e3\uff1aConv1D\u80af\u5b9a\u4f1a\u5bf9\u8f93\u5165\u7684 batch_size \u6761\u8bb0\u5f55\u4e2d\u7684\u6bcf\u4e00\u6761\u90fd\u6267\u884c\u7cfb\u7edf\u7684\u5377\u79ef\u8fc7\u7a0b\uff1b","title":"keras Layers Convolutional Layers"},{"location":"Programming/software-Keras/keras-Layers-Convolutional-Layers/#convolutional-layers","text":"","title":"Convolutional Layers"},{"location":"Programming/software-Keras/keras-Layers-Convolutional-Layers/#conv1d","text":"keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) 1D convolution layer (e.g. temporal convolution ). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None , does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\" , or (None, 128) for variable-length sequences with 128 features per step. Arguments filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . SUMMARY : \u5173\u4e8e filters \u548c kernel_size \uff0c\u53c2\u89c1\u4e0b\u9762\u8fd9\u7bc7\u6587\u7ae0\uff1a Keras conv1d layer parameters: filters and kernel_size SUMMARY : \u5173\u4e8echannel-last\u548cchannel-first\uff0c\u53c2\u89c1\u8fd9\u7bc7\u6587\u7ae0\uff1a A Gentle Introduction to Channels-First and Channels-Last Image Formats","title":"Conv1D"},{"location":"Programming/software-Keras/keras-Layers-Convolutional-Layers/#input-shape","text":"3D tensor with shape: (batch, steps, channels) SUMMARY : \u8981\u60f3\u7406\u89e3\u8fd9\u6bb5\u8bdd\u4e2d steps \u3001 channels \u7684\u542b\u4e49\uff0c\u9996\u5148\u9700\u8981\u4ed4\u7ec6\u9605\u8bfb\u4e0a\u9762\u7684\u7b2c\u4e09\u6bb5\uff0c\u5176\u4e2d\u5df2\u7ecf\u7ed9\u51fa\u4e86\u4e00\u4e2aexample\uff1b\u8fd9\u91cc\u6211\u518d\u8865\u5145\u4e00\u4e2a\u4f8b\u5b50\uff1a \u5982\u679c\u4ee5 Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks \u4e2d\u7684sentence\u4e3a\u4f8b\uff0c\u90a3\u4e48 steps \u5219\u8868\u793a\u7684sentence\u7684\u957f\u5ea6\uff0c\u5373sentence\u4e2dword\u7684\u4e2a\u6570\uff1b channels \u5219\u8868\u793aword embedding+position embedding\u7684\u957f\u5ea6\uff1b","title":"Input  shape"},{"location":"Programming/software-Keras/keras-Layers-Convolutional-Layers/#output-shape","text":"3D tensor with shape: (batch, new_steps, filters) steps value might have changed due to padding or strides. SUMMARY : \u4e0a\u8ff0output shape\u548c Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks \u4e2d\u63cf\u8ff0\u7684\u4e0d\u540c\uff1b SUMMARY : \u4e00\u822c\u5728\u8bb2\u89e3model\u7684\u539f\u7406\u65f6\u5019\u90fd\u662f\u4e0d\u4f1a\u6d89\u53ca\u5230 batch_size \u7684\uff0c\u800c\u662f\u4ec5\u4ec5\u4e00\u4e00\u6761\u8f93\u5165\u6570\u636e\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u4f46\u662f\u5b9e\u73b0\u5e93\u4e2d\u5219\u5fc5\u987b\u8981\u6d89\u53ca\u5230 batch_size \uff0c\u8fd9\u91cc\u4fbf\u662f\u8fd9\u6837\uff1b\u5176\u5b9e\u6211\u89c9\u5f97\u5e94\u8be5\u8fd9\u6837\u6765\u7406\u89e3\uff1aConv1D\u80af\u5b9a\u4f1a\u5bf9\u8f93\u5165\u7684 batch_size \u6761\u8bb0\u5f55\u4e2d\u7684\u6bcf\u4e00\u6761\u90fd\u6267\u884c\u7cfb\u7edf\u7684\u5377\u79ef\u8fc7\u7a0b\uff1b","title":"Output shape"},{"location":"Programming/software-pytorch/paper-Automatic-differentiation-in-PyTorch/","text":"","title":"paper Automatic differentiation in PyTorch"},{"location":"Research-Institution&Researcher/Research-institution/","text":"\u7814\u7a76\u673a\u6784 # \u4e2d\u56fd\u79d1\u5b66\u9662\u81ea\u52a8\u5316\u7814\u7a76\u6240 #","title":"Research-institution"},{"location":"Research-Institution&Researcher/Research-institution/#_1","text":"","title":"\u7814\u7a76\u673a\u6784"},{"location":"Research-Institution&Researcher/Research-institution/#_2","text":"","title":"\u4e2d\u56fd\u79d1\u5b66\u9662\u81ea\u52a8\u5316\u7814\u7a76\u6240"},{"location":"Research-Institution&Researcher/Researcher/","text":"\u7814\u7a76\u5458 # \u8d75\u519b # http://people.ucas.edu.cn/~zhaojun Michael Nielsen # Neural Networks and Deep Learning LiHang # Recent Progress in Deep Learning for Natural Language Processing","title":"Researcher"},{"location":"Research-Institution&Researcher/Researcher/#_1","text":"","title":"\u7814\u7a76\u5458"},{"location":"Research-Institution&Researcher/Researcher/#_2","text":"http://people.ucas.edu.cn/~zhaojun","title":"\u8d75\u519b"},{"location":"Research-Institution&Researcher/Researcher/#michael-nielsen","text":"Neural Networks and Deep Learning","title":"Michael Nielsen"},{"location":"Research-Institution&Researcher/Researcher/#lihang","text":"Recent Progress in Deep Learning for Natural Language Processing","title":"LiHang"},{"location":"Theory/","text":"\u524d\u8a00 # \u673a\u5668\u5b66\u4e60\u7684\u7406\u8bba\u77e5\u8bc6\u3002","title":"Introduction"},{"location":"Theory/#_1","text":"\u673a\u5668\u5b66\u4e60\u7684\u7406\u8bba\u77e5\u8bc6\u3002","title":"\u524d\u8a00"},{"location":"Theory/Data-generating-process/","text":"Data-generating process # \u5728\u591a\u4e2a\u5730\u65b9\u9047\u5230\u4e86\u8fd9\u4e2a\u8bcd\u8bed\uff0c\u4f46\u662f\u76ee\u524d\u8fd8\u662f\u6ca1\u6709\u5145\u5206\u7406\u89e3\u5b83\u7684\u542b\u4e49\u3002 \u4e0b\u9762\u679a\u4e3e\u4e86\u78b0\u5230\u8be5\u8bcd\u7684\u5730\u65b9\uff1a Statistical model Deep learning book\u76845.2 Capacity, Overfitting and Underfitting","title":"Data-generating-process"},{"location":"Theory/Data-generating-process/#data-generating-process","text":"\u5728\u591a\u4e2a\u5730\u65b9\u9047\u5230\u4e86\u8fd9\u4e2a\u8bcd\u8bed\uff0c\u4f46\u662f\u76ee\u524d\u8fd8\u662f\u6ca1\u6709\u5145\u5206\u7406\u89e3\u5b83\u7684\u542b\u4e49\u3002 \u4e0b\u9762\u679a\u4e3e\u4e86\u78b0\u5230\u8be5\u8bcd\u7684\u5730\u65b9\uff1a Statistical model Deep learning book\u76845.2 Capacity, Overfitting and Underfitting","title":"Data-generating process"},{"location":"Theory/VS-statistical-model-VS-machine-learning-model/","text":"VS: statistical model VS machine learning model # Statistical model \u4e2d\u5173\u4e8estatistical model\u7684\u96be\u70b9\uff1a Choosing an appropriate statistical model to represent a given data-generating process is sometimes extremely difficult, and may require knowledge of both the process and relevant statistical analyses. Relatedly, the statistician Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\". Statistical model\u7684\u96be\u70b9\u662fmachine learning model\u9700\u8981\u53bb\u89e3\u51b3\u7684\u3002 Statistical model VS machine learning # Deep learning book\u7684Chapter 5 Machine Learning Basics\u4e2d\u7528regression\uff08\u4e00\u79cdstatistical model\uff09\u5bfc\u5f15\u4ecb\u7ecd\u4e86machine learning\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u5728\u540e\u9762\u5b66\u4e60\u5b8c\u4e86deep learning\u7684\u7406\u8bba\u540e\uff0c\u5bf9\u6bd4\u6765\u770bChapter 5 Machine Learning Basics\u4e2d\u5173\u4e8eregression\u7684\u4ecb\u7ecd\uff0c\u6709\u5982\u4e0b\u53d1\u73b0\uff1a deep learning\u548cregression\u4e4b\u95f4\u6709\u7740\u672c\u8d28\u7684\u5dee\u522b\uff1a regression\u6b63\u5982\u5728chapter 5.2 Capacity, Overfitting and Underfitting\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a\u5b83\u4f1a\u9650\u5236model\u7684 hypothesis space One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model\u2019s capacity. A polynomial of degree one gives us the linear regression model with which we are already familiar, with prediction $ y=b+wx.$ \u663e\u7136\u5728linear regression\u4e2d\uff0c\u9700\u8981\u5b66\u4e60\u7684\u53c2\u6570\u662f b \u548c w \u4f46\u662fdeep learning\u5219\u4e0d\u540c\uff0c\u5b83\u5e76\u4e0d\u4f1a\u9650\u5236model\u7684 hypothesis space \uff0c\u800c\u662f\u53bbapproximate\uff0c\u4e5f\u5c31\u662f universal approximation theorem \u4e2d\u6240\u63cf\u8ff0\u7684\u90a3\u6837\uff1b deep learning\u548cregression\u4e4b\u95f4\u7684\u5171\u540c\u70b9\uff1a \u672c\u8d28\u4e0a\u6765\u8bf4deep learning\u548cregression\u9700\u8981\u5b66\u4e60\u7684\u90fd\u662fparameter\uff1b\u4e0d\u7ba1\u662f\u591a\u4e48\u590d\u6742\u7684model\uff08MLP\uff0cCNN\uff0cRNN\uff0cLSTM\u7b49\u7b49\uff09\uff0c\u5b83\u6700\u7ec8\u90fd\u662f\u7531\u4e00\u7cfb\u5217\u7684parameter\u6765\u51b3\u5b9a\uff1b","title":"VS-statistical-model-VS-machine-learning-model"},{"location":"Theory/VS-statistical-model-VS-machine-learning-model/#vs-statistical-model-vs-machine-learning-model","text":"Statistical model \u4e2d\u5173\u4e8estatistical model\u7684\u96be\u70b9\uff1a Choosing an appropriate statistical model to represent a given data-generating process is sometimes extremely difficult, and may require knowledge of both the process and relevant statistical analyses. Relatedly, the statistician Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\". Statistical model\u7684\u96be\u70b9\u662fmachine learning model\u9700\u8981\u53bb\u89e3\u51b3\u7684\u3002","title":"VS: statistical model VS machine learning model"},{"location":"Theory/VS-statistical-model-VS-machine-learning-model/#statistical-model-vs-machine-learning","text":"Deep learning book\u7684Chapter 5 Machine Learning Basics\u4e2d\u7528regression\uff08\u4e00\u79cdstatistical model\uff09\u5bfc\u5f15\u4ecb\u7ecd\u4e86machine learning\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u5728\u540e\u9762\u5b66\u4e60\u5b8c\u4e86deep learning\u7684\u7406\u8bba\u540e\uff0c\u5bf9\u6bd4\u6765\u770bChapter 5 Machine Learning Basics\u4e2d\u5173\u4e8eregression\u7684\u4ecb\u7ecd\uff0c\u6709\u5982\u4e0b\u53d1\u73b0\uff1a deep learning\u548cregression\u4e4b\u95f4\u6709\u7740\u672c\u8d28\u7684\u5dee\u522b\uff1a regression\u6b63\u5982\u5728chapter 5.2 Capacity, Overfitting and Underfitting\u4e2d\u6240\u63cf\u8ff0\u7684\uff1a\u5b83\u4f1a\u9650\u5236model\u7684 hypothesis space One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model\u2019s capacity. A polynomial of degree one gives us the linear regression model with which we are already familiar, with prediction $ y=b+wx.$ \u663e\u7136\u5728linear regression\u4e2d\uff0c\u9700\u8981\u5b66\u4e60\u7684\u53c2\u6570\u662f b \u548c w \u4f46\u662fdeep learning\u5219\u4e0d\u540c\uff0c\u5b83\u5e76\u4e0d\u4f1a\u9650\u5236model\u7684 hypothesis space \uff0c\u800c\u662f\u53bbapproximate\uff0c\u4e5f\u5c31\u662f universal approximation theorem \u4e2d\u6240\u63cf\u8ff0\u7684\u90a3\u6837\uff1b deep learning\u548cregression\u4e4b\u95f4\u7684\u5171\u540c\u70b9\uff1a \u672c\u8d28\u4e0a\u6765\u8bf4deep learning\u548cregression\u9700\u8981\u5b66\u4e60\u7684\u90fd\u662fparameter\uff1b\u4e0d\u7ba1\u662f\u591a\u4e48\u590d\u6742\u7684model\uff08MLP\uff0cCNN\uff0cRNN\uff0cLSTM\u7b49\u7b49\uff09\uff0c\u5b83\u6700\u7ec8\u90fd\u662f\u7531\u4e00\u7cfb\u5217\u7684parameter\u6765\u51b3\u5b9a\uff1b","title":"Statistical model VS machine learning"},{"location":"Theory/VS-statistical-model-VS-stochastic-process/","text":"statis","title":"VS-statistics-model-VS-stochastic-process"},{"location":"Theory/CRF/crf/","text":"Performing Sequence Labelling using CRF in Python # Conditional random field #","title":"CRF"},{"location":"Theory/CRF/crf/#performing-sequence-labelling-using-crf-in-python","text":"","title":"Performing Sequence Labelling using CRF in Python"},{"location":"Theory/CRF/crf/#conditional-random-field","text":"","title":"Conditional random field"},{"location":"Theory/CRF/paper-Conditional-Random-Fields-Probabilistic-Models/","text":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data #","title":"paper Conditional Random Fields Probabilistic Models"},{"location":"Theory/CRF/paper-Conditional-Random-Fields-Probabilistic-Models/#conditional-random-fields-probabilistic-models-for-segmenting-and-labeling-sequence-data","text":"","title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"},{"location":"Theory/Deep-learning/Attention/Papers/","text":"\u57fa\u4e8etransformer\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff1a - GPT -BERT","title":"Papers"},{"location":"Theory/Deep-learning/Book-deep-learning/Artificial-neural-network/","text":"Artificial neural network Artificial neural network #","title":"Artificial-neural-network"},{"location":"Theory/Deep-learning/Book-deep-learning/Artificial-neural-network/#artificial-neural-network","text":"","title":"Artificial neural network"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-1-Introduction/","text":"Chapter 1 Introduction # In the early days of artificial intelligence, the field rapidly tackled and solved problems that are intellectually difficult for human beings but relatively straightforward for computers\u2014 problems that can be described by a list of formal, mathematical rules . The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally \u2014problems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images. \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u7ae0\u4e2d\uff0c\u4f5c\u8005\u9891\u7e41\u7684\u4f7f\u7528\u4e86 formal \u8fd9\u4e2a\u8bcd\u8bed\uff0c\u6240\u4ee5\u7406\u89e3\u8fd9\u4e2a\u8bcd\u8bed\u5bf9\u4e8e\u7406\u89e3\u672c\u6587\u662f\u81f3\u5173\u91cd\u8981\u7684\uff0c\u8bfb\u8005\u53ef\u4ee5\u5c06\u5b83\u7b80\u5355\u7684\u7406\u89e3\u4e3a\u53ef\u4ee5\u89c4\u5219\u5316\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u8fdb\u884c\u63cf\u8ff0\u7684\u3002\u6709\u7ecf\u9a8c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e00\u5b9a\u80fd\u591f\u60f3\u5230\uff0c problems that can be described by a list of formal, mathematical rules \u662f\u6bd4\u8f83\u5bb9\u6613\u7528\u7a0b\u5e8f\u6765\u8fdb\u884c\u5b9e\u73b0\u7684\u3002 \u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u6240\u64c5\u957f\u7684\u662f\u4e0d\u540c\u7684\uff1a\u8ba1\u7b97\u673a\u6240\u64c5\u957f\u7684\u662f\u89e3\u51b3\u5982\u4e0b\u7c7b\u578b\u7684\u95ee\u9898\uff1a problems that can be described by a list of formal, mathematical rules \u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7b97\u6cd5\u57fa\u672c\u80fd\u591f\u5b9e\u73b0\u3002 \u4eba\u7c7b\u6240\u64c5\u957f\u7684\u662f\u89e3\u51b3\u7684\u662f: problems that are intuitive and hard for people to describe formally \u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7b97\u6cd5\u662f\u975e\u5e38\u96be\u4ee5\u5b9e\u73b0\u7684\u3002\u8fd9\u7c7b\u95ee\u9898\u6b63\u662f\u672c\u4e66\u6240\u63cf\u8ff0\u7684deep learning\u6280\u672f\u5fd7\u4e8e\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u601d\u60f3\u5982\u4e0b\uff1a This book is about a solution to these more intuitive problems . This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined in terms of its relation to simpler concepts . By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all of the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u5f53\u524d machine learning \u7684\u6838\u5fc3\u601d\u60f3\uff0c\u8fd9\u4e2a\u601d\u60f3\u7684\u5bf9\u4e8e\u7406\u89e3\u540e\u9762\u7ae0\u8282\u7684\u5185\u5bb9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u6216\u8005\u8bf4\uff0c\u540e\u9762\u7ae0\u8282\u5c31\u662f\u5728\u544a\u8bc9\u8bfb\u8005\u5982\u4f55\u6765\u5b9e\u73b0machine learning\u3002 \u539f\u4e66\u7684\u8fd9\u4e00\u8282\u7684\u540e\u7eed\u5185\u5bb9\u57fa\u672c\u4e0a\u5c31\u662f\u56f4\u7ed5\u7740\u6211\u4e0a\u9762\u6240\u603b\u7ed3\u7684\u5185\u5bb9\u5c55\u5f00\u7684\u3002 \u6848\u4f8b\uff1aFormal language and natural language # \u5173\u4e8e\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u6240\u64c5\u957f\u89e3\u51b3\u7684\u95ee\u9898\u7684\u4e00\u4e2a\u5178\u578b\u6848\u4f8b\u5c31\u662f\uff1aformal language\u548cnatural language\u3002\u8ba1\u7b97\u673a\u80fd\u591f\u8f7b\u677e\u7406\u89e3formal language\uff08\u56e0\u4e3aformal language\u7684grammar\u548csemantic\u90fd\u80fd\u591f\u5f62\u5f0f\u5316\u5730\u8fdb\u884c\u63cf\u8ff0\uff09\uff0c\u4f46\u662f\u5bf9\u4e8enatural language\u7684\u7406\u89e3\u5219\u662f\u975e\u5e38\u53ef\u80fd\u7684\uff08natural language\u7684grammar\u548csemantic\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u65e0\u6cd5\u8fdb\u884c\u5f62\u5f0f\u5316\u7684\u63cf\u8ff0\uff09\u3002\u800c\u4eba\u7c7b\u57fa\u672c\u76f8\u53cd\u3002 \u6848\u4f8b\uff1a Pattern matching and Pattern recognition # \u8fd9\u662f\u53e6\u5916\u4e00\u4e2a\u6848\u4f8b\u3002 \u5173\u4e8emachine learning\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u53c2\u89c1chapter 5.1 Learning Algorithms\u3002","title":"Chapter-1-Introduction"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-1-Introduction/#chapter-1-introduction","text":"In the early days of artificial intelligence, the field rapidly tackled and solved problems that are intellectually difficult for human beings but relatively straightforward for computers\u2014 problems that can be described by a list of formal, mathematical rules . The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally \u2014problems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images. \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u672c\u7ae0\u4e2d\uff0c\u4f5c\u8005\u9891\u7e41\u7684\u4f7f\u7528\u4e86 formal \u8fd9\u4e2a\u8bcd\u8bed\uff0c\u6240\u4ee5\u7406\u89e3\u8fd9\u4e2a\u8bcd\u8bed\u5bf9\u4e8e\u7406\u89e3\u672c\u6587\u662f\u81f3\u5173\u91cd\u8981\u7684\uff0c\u8bfb\u8005\u53ef\u4ee5\u5c06\u5b83\u7b80\u5355\u7684\u7406\u89e3\u4e3a\u53ef\u4ee5\u89c4\u5219\u5316\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528\u6570\u5b66\u516c\u5f0f\u8fdb\u884c\u63cf\u8ff0\u7684\u3002\u6709\u7ecf\u9a8c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e00\u5b9a\u80fd\u591f\u60f3\u5230\uff0c problems that can be described by a list of formal, mathematical rules \u662f\u6bd4\u8f83\u5bb9\u6613\u7528\u7a0b\u5e8f\u6765\u8fdb\u884c\u5b9e\u73b0\u7684\u3002 \u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u6240\u64c5\u957f\u7684\u662f\u4e0d\u540c\u7684\uff1a\u8ba1\u7b97\u673a\u6240\u64c5\u957f\u7684\u662f\u89e3\u51b3\u5982\u4e0b\u7c7b\u578b\u7684\u95ee\u9898\uff1a problems that can be described by a list of formal, mathematical rules \u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7b97\u6cd5\u57fa\u672c\u80fd\u591f\u5b9e\u73b0\u3002 \u4eba\u7c7b\u6240\u64c5\u957f\u7684\u662f\u89e3\u51b3\u7684\u662f: problems that are intuitive and hard for people to describe formally \u5bf9\u4e8e\u8fd9\u7c7b\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u7b97\u6cd5\u662f\u975e\u5e38\u96be\u4ee5\u5b9e\u73b0\u7684\u3002\u8fd9\u7c7b\u95ee\u9898\u6b63\u662f\u672c\u4e66\u6240\u63cf\u8ff0\u7684deep learning\u6280\u672f\u5fd7\u4e8e\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u601d\u60f3\u5982\u4e0b\uff1a This book is about a solution to these more intuitive problems . This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined in terms of its relation to simpler concepts . By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all of the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u5f53\u524d machine learning \u7684\u6838\u5fc3\u601d\u60f3\uff0c\u8fd9\u4e2a\u601d\u60f3\u7684\u5bf9\u4e8e\u7406\u89e3\u540e\u9762\u7ae0\u8282\u7684\u5185\u5bb9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u6216\u8005\u8bf4\uff0c\u540e\u9762\u7ae0\u8282\u5c31\u662f\u5728\u544a\u8bc9\u8bfb\u8005\u5982\u4f55\u6765\u5b9e\u73b0machine learning\u3002 \u539f\u4e66\u7684\u8fd9\u4e00\u8282\u7684\u540e\u7eed\u5185\u5bb9\u57fa\u672c\u4e0a\u5c31\u662f\u56f4\u7ed5\u7740\u6211\u4e0a\u9762\u6240\u603b\u7ed3\u7684\u5185\u5bb9\u5c55\u5f00\u7684\u3002","title":"Chapter 1 Introduction"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-1-Introduction/#formal-language-and-natural-language","text":"\u5173\u4e8e\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u6240\u64c5\u957f\u89e3\u51b3\u7684\u95ee\u9898\u7684\u4e00\u4e2a\u5178\u578b\u6848\u4f8b\u5c31\u662f\uff1aformal language\u548cnatural language\u3002\u8ba1\u7b97\u673a\u80fd\u591f\u8f7b\u677e\u7406\u89e3formal language\uff08\u56e0\u4e3aformal language\u7684grammar\u548csemantic\u90fd\u80fd\u591f\u5f62\u5f0f\u5316\u5730\u8fdb\u884c\u63cf\u8ff0\uff09\uff0c\u4f46\u662f\u5bf9\u4e8enatural language\u7684\u7406\u89e3\u5219\u662f\u975e\u5e38\u53ef\u80fd\u7684\uff08natural language\u7684grammar\u548csemantic\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u65e0\u6cd5\u8fdb\u884c\u5f62\u5f0f\u5316\u7684\u63cf\u8ff0\uff09\u3002\u800c\u4eba\u7c7b\u57fa\u672c\u76f8\u53cd\u3002","title":"\u6848\u4f8b\uff1aFormal language and natural language"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-1-Introduction/#pattern-matching-and-pattern-recognition","text":"\u8fd9\u662f\u53e6\u5916\u4e00\u4e2a\u6848\u4f8b\u3002 \u5173\u4e8emachine learning\u80fd\u591f\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u53c2\u89c1chapter 5.1 Learning Algorithms\u3002","title":"\u6848\u4f8b\uff1aPattern matching and Pattern recognition"},{"location":"Theory/Deep-learning/Book-deep-learning/Deep-learning-book/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u662f\u6211\u9605\u8bfb Deep Learning Book \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 https://github.com/janishar/mit-deep-learning-book-pdf","title":"Introduction"},{"location":"Theory/Deep-learning/Book-deep-learning/Deep-learning-book/#_1","text":"\u672c\u7ae0\u662f\u6211\u9605\u8bfb Deep Learning Book \u7684\u9605\u8bfb\u7b14\u8bb0\u3002 https://github.com/janishar/mit-deep-learning-book-pdf","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/5.2-Capacity-Overfitting-and-Underfitting/","text":"5.2 Capacity, Overfitting and Underfitting # The train and test data are generated by a probability distribution over datasets called the data generating process . We typically make a set of assumptions known collectively as the i.i.d. assumptions . These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed , drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution , denoted $p_{data}$ . This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error. \u5173\u4e8e i.i.d. assumptions \uff0c\u53c2\u89c1 Independent and identically distributed random variables \u3002 \u201cprobability distribution\u201d\u662f Probability theory \u4e2d\u7684\u6982\u5ff5\uff0c\u5b83\u6240\u6307\u4e3a probability distributions \u3002\u6240\u8c13\u201c identically distributed \u201d\uff0c\u662f\u6307\u4e24\u79cd\u7684 probability distributions \u76f8\u540c\u3002","title":"5.2-Capacity-Overfitting-and-Underfitting"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/5.2-Capacity-Overfitting-and-Underfitting/#52-capacity-overfitting-and-underfitting","text":"The train and test data are generated by a probability distribution over datasets called the data generating process . We typically make a set of assumptions known collectively as the i.i.d. assumptions . These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed , drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution , denoted $p_{data}$ . This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error. \u5173\u4e8e i.i.d. assumptions \uff0c\u53c2\u89c1 Independent and identically distributed random variables \u3002 \u201cprobability distribution\u201d\u662f Probability theory \u4e2d\u7684\u6982\u5ff5\uff0c\u5b83\u6240\u6307\u4e3a probability distributions \u3002\u6240\u8c13\u201c identically distributed \u201d\uff0c\u662f\u6307\u4e24\u79cd\u7684 probability distributions \u76f8\u540c\u3002","title":"5.2 Capacity, Overfitting and Underfitting"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Chapter-5-Machine-Learning-Basics/","text":"Chapter 5 Machine Learning Basics # \u6bcf\u7ae0\u7684\u5f00\u5934\u4f5c\u8005\u90fd\u4f1a\u629b\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u672c\u7ae0\u4f5c\u8005\u6240\u629b\u51fa\u7684\u95ee\u9898\u53ef\u4ee5\u8bf4\u662fmachine learning\u4e2d\u6700\u6700\u672c\u8d28\u7684\u95ee\u9898\uff1a What a learning algorithm is How the challenge of fitting the training data differs from the challenge of finding patterns that generalize to new data How to set hyperparameters Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions; we therefore present the two central approaches to statistics: frequentist estimators and Bayesian inference. \u8fd9\u6bb5\u8bdd\u5982\u4f55\u7406\u89e3\uff1f \u672c\u8d28\u4e0a\u6765\u8bf4\uff0c\u201cmachine learning\u201d\u5c5e\u201capplied statistics\u201d\u3002\u6240\u4ee5\u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u9700\u8981\u5bf9 Statistics \u7684\u7814\u7a76\u5206\u652f\u6709\u4e00\u4e9b\u4e86\u89e3\u4e86\u3002\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684frequentist estimators \u548c Bayesian inference\u90fd\u5c5e\u4e8e Statistical inference \u8303\u8f74\uff0c\u6309\u7167 Statistical inference \u7684\u8bf4\u6cd5\uff0c\u201cfrequentist estimators \u201d\u5bf9\u5e94\u7684\u662f Frequentist inference \uff0c\u201cBayesian inference\u201d\u5bf9\u5e94\u7684\u662f Bayesian inference \uff0c\u5b83\u4eec\u662f Frequentist inference \u7684\u4e24\u4e2a\u5b66\u6d3e\uff08\u6216paradigm\uff09\uff0c\u5173\u4e8e Frequentist inference \u7684\u5b66\u6d3e\uff0c\u53c2\u89c1 Paradigms for inference \u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a These schools\u2014or \"paradigms\"\u2014are not mutually exclusive, and methods that work well under one paradigm often have attractive interpretations under other paradigms. \u201cconfidence intervals\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\uff1a\u7f6e\u4fe1\u533a\u95f4\uff0c\u53c2\u89c1 Confidence Interval \u3002 \u6709\u8fd9\u4e9b\u8ba4\u77e5\u5bf9\u4e8e\u7406\u89e3\u540e\u7eed\u7ae0\u8282\u7684\u5185\u5bb9\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 We describe how to combine various algorithm components such as an optimization algorithm , a cost function , a model , and a dataset to build a machine learning algorithm . \u6211\u89c9\u5f97\u8fd9\u6bb5\u8bdd\u6240\u4f20\u8fbe\u7684\u601d\u60f3\u662f\u6bd4\u8f83\u597d\u7684\uff1a\u5982\u679c\u5c06machine learning algorithm\u770b\u505a\u662f\u4e00\u4e2a\u673a\u5668\u7684\uff0c\u90a3\u4e48\u5b83\u6709\u5982\u4e0b\u96f6\u4ef6\uff08component\uff09\u7ec4\u6210\uff1a optimization algorithm model dataset \u90a3\u8bfb\u8005\u770b\u5230\u4f1a\u63d0\u51fa\u8fd9\u6837\u7684\u95ee\u9898\uff1a \u8fd9\u4e9bcomponent\u5206\u522b\u8868\u793a\u7684\u662f\u4ec0\u4e48\uff1f \u5b83\u4eec\u4e4b\u95f4\u662f\u5982\u4f55\u7ec4\u88c5\u3001\u534f\u52a9\u6765\u6784\u6210\u4e00\u4e2a\u5b8c\u6574\u7684machine learning algorithm\uff1f \u6bcf\u4e2acomponent\u6709\u54ea\u4e9b\u53ef\u4f9b\u9009\u62e9\u7684option\uff1f \u8fd9\u4e9b\u95ee\u9898\u5728\u672c\u4e66\u7684\u540e\u7eed\u7ae0\u8282\u4f1a\u4e13\u95e8\u8fdb\u884c\u4ecb\u7ecd\u3002 Finally, in section , we describe some of the 5.11 factors that have limited the ability of traditional machine learning to generalize. These challenges have motivated the development of deep learning algorithms that overcome these obstacles. \u4e0a\u8ff0ability\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1fdeep learning algorithm\u8f83traditional machine learning algorithm\u7684\u4f18\u52bf\u4f55\u5728\uff1f 5.1 Learning Algorithms # \u672c\u6bb5\u4e2d\u5173\u4e8e\u201clearning\u201d\u7684\u5b9a\u4e49\u662f\u5f15\u7528\u7684\u5982\u4e0b\u4e66\u7c4d\uff1a Machine Learning , Tom Mitchell , McGraw Hill, 1997. 5.1.1 The Task, T # \u672c\u8282\u4f5c\u8005\u6240\u8981\u8868\u8fbe\u7684\u4e3b\u8981\u601d\u60f3\u7b80\u5355\u63cf\u8ff0\u5982\u4e0b\uff1a \u4eba\u7c7b\u5f00\u53d1computer program\u6765\u89e3\u51b3\u5f62\u5f62\u8272\u8272\u7684problem\uff0c\u8fd9\u4e9bproblem\u5c31\u662fprogram\u6240\u8981\u6267\u884c\u7684task\uff0c\u4f46\u662f\u6211\u4eec\u77e5\u9053\uff0cprogram\u5e76\u975e\u4e07\u80fd\u7684\uff0c\u8fd8\u662f\u6709\u975e\u5e38\u975e\u5e38\u591a\u7684problem\u662f\u65e0\u6cd5\u4f7f\u7528program\u6765\u89e3\u51b3\u7684\u3002\u968f\u7740\u79d1\u6280\u7684\u53d1\u5c55\uff0cprogram\u80fd\u591f\u89e3\u51b3\u7684problem\u4e5f\u8d8a\u6765\u8d8a\u591a\u4e86\uff0c\u4e5f\u5c31\u662fprogram\u7684\u80fd\u529b\u8d8a\u6765\u8d8a\u5f3a\u4e86\u3002machine learning algorithm\u5c31\u662f\u4e00\u79cd\u5728\u4e00\u7c7btask\u4e2d \u601d\u8003\uff1amachine learning algorithm VS \u666e\u901aalgorithm\uff1f machine learning algorithm\u662f\u4e00\u79cd\u5168\u65b0\u7684\u7b97\u6cd5\u8303\u5f0f\uff0c\u5b83\u4f7fprogram\u80fd\u591f\u201clearning\u201d\uff08\u201clearning\u201d\u57285.1 Learning Algorithms\u4e2d\u7ed9\u51fa\u5b9a\u4e49\uff09 Learning is our means of attaining the ability to perform the task. Pattern recognition\u662f\u4e00\u7c7b\u975e\u5e38\u666e\u904d\u7684task\uff0c\u5728\u4e0b\u8282\u4e2d\u4f1a\u8fdb\u884c\u8ba8\u8bba\u3002","title":"Chapter-5-Machine-Learning-Basics"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Chapter-5-Machine-Learning-Basics/#chapter-5-machine-learning-basics","text":"\u6bcf\u7ae0\u7684\u5f00\u5934\u4f5c\u8005\u90fd\u4f1a\u629b\u51fa\u4e00\u7cfb\u5217\u7684\u95ee\u9898\uff0c\u672c\u7ae0\u4f5c\u8005\u6240\u629b\u51fa\u7684\u95ee\u9898\u53ef\u4ee5\u8bf4\u662fmachine learning\u4e2d\u6700\u6700\u672c\u8d28\u7684\u95ee\u9898\uff1a What a learning algorithm is How the challenge of fitting the training data differs from the challenge of finding patterns that generalize to new data How to set hyperparameters Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions; we therefore present the two central approaches to statistics: frequentist estimators and Bayesian inference. \u8fd9\u6bb5\u8bdd\u5982\u4f55\u7406\u89e3\uff1f \u672c\u8d28\u4e0a\u6765\u8bf4\uff0c\u201cmachine learning\u201d\u5c5e\u201capplied statistics\u201d\u3002\u6240\u4ee5\u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u9700\u8981\u5bf9 Statistics \u7684\u7814\u7a76\u5206\u652f\u6709\u4e00\u4e9b\u4e86\u89e3\u4e86\u3002\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684frequentist estimators \u548c Bayesian inference\u90fd\u5c5e\u4e8e Statistical inference \u8303\u8f74\uff0c\u6309\u7167 Statistical inference \u7684\u8bf4\u6cd5\uff0c\u201cfrequentist estimators \u201d\u5bf9\u5e94\u7684\u662f Frequentist inference \uff0c\u201cBayesian inference\u201d\u5bf9\u5e94\u7684\u662f Bayesian inference \uff0c\u5b83\u4eec\u662f Frequentist inference \u7684\u4e24\u4e2a\u5b66\u6d3e\uff08\u6216paradigm\uff09\uff0c\u5173\u4e8e Frequentist inference \u7684\u5b66\u6d3e\uff0c\u53c2\u89c1 Paradigms for inference \u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a These schools\u2014or \"paradigms\"\u2014are not mutually exclusive, and methods that work well under one paradigm often have attractive interpretations under other paradigms. \u201cconfidence intervals\u201d\u7684\u4e2d\u6587\u610f\u601d\u662f\uff1a\u7f6e\u4fe1\u533a\u95f4\uff0c\u53c2\u89c1 Confidence Interval \u3002 \u6709\u8fd9\u4e9b\u8ba4\u77e5\u5bf9\u4e8e\u7406\u89e3\u540e\u7eed\u7ae0\u8282\u7684\u5185\u5bb9\u662f\u6bd4\u8f83\u91cd\u8981\u7684\u3002 We describe how to combine various algorithm components such as an optimization algorithm , a cost function , a model , and a dataset to build a machine learning algorithm . \u6211\u89c9\u5f97\u8fd9\u6bb5\u8bdd\u6240\u4f20\u8fbe\u7684\u601d\u60f3\u662f\u6bd4\u8f83\u597d\u7684\uff1a\u5982\u679c\u5c06machine learning algorithm\u770b\u505a\u662f\u4e00\u4e2a\u673a\u5668\u7684\uff0c\u90a3\u4e48\u5b83\u6709\u5982\u4e0b\u96f6\u4ef6\uff08component\uff09\u7ec4\u6210\uff1a optimization algorithm model dataset \u90a3\u8bfb\u8005\u770b\u5230\u4f1a\u63d0\u51fa\u8fd9\u6837\u7684\u95ee\u9898\uff1a \u8fd9\u4e9bcomponent\u5206\u522b\u8868\u793a\u7684\u662f\u4ec0\u4e48\uff1f \u5b83\u4eec\u4e4b\u95f4\u662f\u5982\u4f55\u7ec4\u88c5\u3001\u534f\u52a9\u6765\u6784\u6210\u4e00\u4e2a\u5b8c\u6574\u7684machine learning algorithm\uff1f \u6bcf\u4e2acomponent\u6709\u54ea\u4e9b\u53ef\u4f9b\u9009\u62e9\u7684option\uff1f \u8fd9\u4e9b\u95ee\u9898\u5728\u672c\u4e66\u7684\u540e\u7eed\u7ae0\u8282\u4f1a\u4e13\u95e8\u8fdb\u884c\u4ecb\u7ecd\u3002 Finally, in section , we describe some of the 5.11 factors that have limited the ability of traditional machine learning to generalize. These challenges have motivated the development of deep learning algorithms that overcome these obstacles. \u4e0a\u8ff0ability\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1fdeep learning algorithm\u8f83traditional machine learning algorithm\u7684\u4f18\u52bf\u4f55\u5728\uff1f","title":"Chapter 5 Machine Learning Basics"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Chapter-5-Machine-Learning-Basics/#51-learning-algorithms","text":"\u672c\u6bb5\u4e2d\u5173\u4e8e\u201clearning\u201d\u7684\u5b9a\u4e49\u662f\u5f15\u7528\u7684\u5982\u4e0b\u4e66\u7c4d\uff1a Machine Learning , Tom Mitchell , McGraw Hill, 1997.","title":"5.1 Learning Algorithms"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Chapter-5-Machine-Learning-Basics/#511-the-task-t","text":"\u672c\u8282\u4f5c\u8005\u6240\u8981\u8868\u8fbe\u7684\u4e3b\u8981\u601d\u60f3\u7b80\u5355\u63cf\u8ff0\u5982\u4e0b\uff1a \u4eba\u7c7b\u5f00\u53d1computer program\u6765\u89e3\u51b3\u5f62\u5f62\u8272\u8272\u7684problem\uff0c\u8fd9\u4e9bproblem\u5c31\u662fprogram\u6240\u8981\u6267\u884c\u7684task\uff0c\u4f46\u662f\u6211\u4eec\u77e5\u9053\uff0cprogram\u5e76\u975e\u4e07\u80fd\u7684\uff0c\u8fd8\u662f\u6709\u975e\u5e38\u975e\u5e38\u591a\u7684problem\u662f\u65e0\u6cd5\u4f7f\u7528program\u6765\u89e3\u51b3\u7684\u3002\u968f\u7740\u79d1\u6280\u7684\u53d1\u5c55\uff0cprogram\u80fd\u591f\u89e3\u51b3\u7684problem\u4e5f\u8d8a\u6765\u8d8a\u591a\u4e86\uff0c\u4e5f\u5c31\u662fprogram\u7684\u80fd\u529b\u8d8a\u6765\u8d8a\u5f3a\u4e86\u3002machine learning algorithm\u5c31\u662f\u4e00\u79cd\u5728\u4e00\u7c7btask\u4e2d \u601d\u8003\uff1amachine learning algorithm VS \u666e\u901aalgorithm\uff1f machine learning algorithm\u662f\u4e00\u79cd\u5168\u65b0\u7684\u7b97\u6cd5\u8303\u5f0f\uff0c\u5b83\u4f7fprogram\u80fd\u591f\u201clearning\u201d\uff08\u201clearning\u201d\u57285.1 Learning Algorithms\u4e2d\u7ed9\u51fa\u5b9a\u4e49\uff09 Learning is our means of attaining the ability to perform the task. Pattern recognition\u662f\u4e00\u7c7b\u975e\u5e38\u666e\u904d\u7684task\uff0c\u5728\u4e0b\u8282\u4e2d\u4f1a\u8fdb\u884c\u8ba8\u8bba\u3002","title":"5.1.1 The Task, T"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Pattern-recognition/","text":"Pattern recognition # Sequence labeling","title":"Pattern-recognition"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Pattern-recognition/#pattern-recognition","text":"Sequence labeling","title":"Pattern recognition"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/","text":"Sequence Labeling: Generative and Discriminative Approaches # Sequence labeling problem definition I # Given a sequence of observations/feature vectors, determine an appropriate label/state for each observation We will assume the observations can be discrete or continuous, scalar or vector We assume the labels/states are discrete and from a finite set Try to reduce errors by considering the relations between the observations and states (observation-state) and the relation between neighboring states (state-state) Sequence labeling applications # Speech recognition Part-of-speech tagging Shallow parsing Handwriting recognition Protein secondary structure prediction Video analysis Facial expression dynamic modeling Urns and balls example # Assume there are two urns with black and white balls [Rabiner, 1989]. One urn has more black than white (90% vs 10%) and vice versa. Someone pulls out one ball at a time and shows us without revealing which urn he uses and puts it back into the urn. He is more likely to use the same urn (90% chance) once he starts using one We are looking only at the sequence of balls and recording them. Questions about the urns and balls example # Questions of interest: Can we predict which urn is used at a given time? What is the probability of observing the sequence of balls shown to us? Can we estimate/learn the ratio of balls in each urn by looking at a long sequence of balls if we did not know the ratios beforehand? Jason Eisner\u2019s ice-cream example # Try to guess whether the weather was hot or cold by observing only how many ice-creams (0, 1, 2 or 3+) Jason ate each day in a sequence of 30 days. Two states and observations with 4 distinct values (discrete observations). Question: Can we determine if a day was hot or cold given the sequence of ice-creams consumed by Jason? Example excel sheet online (illustrates forward backward algorithm). Example also adopted in [Jurafsky and Martin, 2008] Approach, notation and variables # We will first analyze binary and multi-class classification with linear models. Multi-class classification will be the basis for understanding the sequence labeling problem . Then, we will introduce HMM, CRF, and structured SVM approaches for sequence labeling. Notation: $x$ is an observed feature vector, $x_t$ a feature vector at sequence position $t$, $x_{1:T}$ a sequence of feature vectors. $y$ is a discrete label (or state), $y \\in Y$ where $Y = {\u22121, +1}$ for binary classification, $Y = [M] = {1, 2, . . . , M}$ for multi-class classification. $y_t$ is the label/state at sequence position $t$, $y_{1:T}$ is a sequence of labels/states $w$ and w\u02dc are parameter vectors,$w_j$ is the $j$th component. $F(x_{1:T} , y_{1:T} )$ is a feature vector for CRF and structured SVM","title":"Sequence-Labeling-Generative-and-Discriminative"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#sequence-labeling-generative-and-discriminative-approaches","text":"","title":"Sequence Labeling: Generative and Discriminative Approaches"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#sequence-labeling-problem-definition-i","text":"Given a sequence of observations/feature vectors, determine an appropriate label/state for each observation We will assume the observations can be discrete or continuous, scalar or vector We assume the labels/states are discrete and from a finite set Try to reduce errors by considering the relations between the observations and states (observation-state) and the relation between neighboring states (state-state)","title":"Sequence labeling problem definition I"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#sequence-labeling-applications","text":"Speech recognition Part-of-speech tagging Shallow parsing Handwriting recognition Protein secondary structure prediction Video analysis Facial expression dynamic modeling","title":"Sequence labeling applications"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#urns-and-balls-example","text":"Assume there are two urns with black and white balls [Rabiner, 1989]. One urn has more black than white (90% vs 10%) and vice versa. Someone pulls out one ball at a time and shows us without revealing which urn he uses and puts it back into the urn. He is more likely to use the same urn (90% chance) once he starts using one We are looking only at the sequence of balls and recording them.","title":"Urns and balls example"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#questions-about-the-urns-and-balls-example","text":"Questions of interest: Can we predict which urn is used at a given time? What is the probability of observing the sequence of balls shown to us? Can we estimate/learn the ratio of balls in each urn by looking at a long sequence of balls if we did not know the ratios beforehand?","title":"Questions about the urns and balls example"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#jason-eisners-ice-cream-example","text":"Try to guess whether the weather was hot or cold by observing only how many ice-creams (0, 1, 2 or 3+) Jason ate each day in a sequence of 30 days. Two states and observations with 4 distinct values (discrete observations). Question: Can we determine if a day was hot or cold given the sequence of ice-creams consumed by Jason? Example excel sheet online (illustrates forward backward algorithm). Example also adopted in [Jurafsky and Martin, 2008]","title":"Jason Eisner\u2019s ice-cream example"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-Labeling-Generative-and-Discriminative/#approach-notation-and-variables","text":"We will first analyze binary and multi-class classification with linear models. Multi-class classification will be the basis for understanding the sequence labeling problem . Then, we will introduce HMM, CRF, and structured SVM approaches for sequence labeling. Notation: $x$ is an observed feature vector, $x_t$ a feature vector at sequence position $t$, $x_{1:T}$ a sequence of feature vectors. $y$ is a discrete label (or state), $y \\in Y$ where $Y = {\u22121, +1}$ for binary classification, $Y = [M] = {1, 2, . . . , M}$ for multi-class classification. $y_t$ is the label/state at sequence position $t$, $y_{1:T}$ is a sequence of labels/states $w$ and w\u02dc are parameter vectors,$w_j$ is the $j$th component. $F(x_{1:T} , y_{1:T} )$ is a feature vector for CRF and structured SVM","title":"Approach, notation and variables"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-labeling/","text":"Sequence labeling #","title":"Sequence-labeling"},{"location":"Theory/Deep-learning/Book-deep-learning/Chapter-5-Machine-Learning-Basics/Task/Sequence-labeling/Sequence-labeling/#sequence-labeling","text":"","title":"Sequence labeling"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/","text":"What does \u201cend to end\u201d mean in deep learning methods? A Limits of End-to-End Learning Abstract Introduction What does \u201cend to end\u201d mean in deep learning methods? # I want to know what it is, and how it is any different from ensembling? Suppose, I want to achieve high accuracy in classification and segmentation, for a specific task, if I use different networks, such as CNN, RNN, etc to achieve this, is this called an end to end model? (architecture?) or not? A # end-to-end = all parameters are trained jointly (vs. step-by-step ) ensembling = several classifiers are trained independently, each classifier makes a prediction, and all predictions are combined into one using some strategy (e.g., take the most common prediction across all classifiers). Limits of End-to-End Learning # Abstract # End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable\uff08\u53ef\u5fae\u7684\uff09. In effect, not only a central learning machine , but also all \u201cperipheral\u201d modules like representation learning and memory formation are covered by a holistic\uff08\u6574\u4f53\u7684\uff09 learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning. Keywords: end-to-end machine learning Introduction # We are today in the position to train rather deep and complex neural networks in an end-to-end (e2e) fashion, by gradient descent. In a nutshell, this amounts to\uff08\u76f8\u5f53\u4e8e\uff09 scaling up the good old backpropagation algorithm (see Schmidhuber, 2015 and references therein) to immensely rich and complex models. However, the end-to-end learning philosophy goes one step further: carefully ensuring that all modules of a learning systems are differentiable with respect to all adjustable parameters (weights) and training this system as a whole are lifted to the status of principles. This elegant although straightforward and somewhat brute-force technique has been popularized in the context of deep learning. It is a seemingly natural consequence of deep neural architectures blurring the classic boundaries between learning machine and other processing components by casting a possibly complex processing pipeline into the coherent and flexible modeling language of neural networks.1 The approach yields state-of-the-art results (Collobert et al., 2011; Krizhevsky et al., 2012; Mnih et al., 2015). Its appeal is a unified training scheme that makes most of the available information by taking labels (supervised learning) and rewards (reinforcement learning) into account, instead of relying only on the input distribution (unsupervised pre-training). Excellent recent examples of studies","title":"Introduction"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/#what-does-end-to-end-mean-in-deep-learning-methods","text":"I want to know what it is, and how it is any different from ensembling? Suppose, I want to achieve high accuracy in classification and segmentation, for a specific task, if I use different networks, such as CNN, RNN, etc to achieve this, is this called an end to end model? (architecture?) or not?","title":"What does \u201cend to end\u201d mean in deep learning methods?"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/#a","text":"end-to-end = all parameters are trained jointly (vs. step-by-step ) ensembling = several classifiers are trained independently, each classifier makes a prediction, and all predictions are combined into one using some strategy (e.g., take the most common prediction across all classifiers).","title":"A"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/#limits-of-end-to-end-learning","text":"","title":"Limits of End-to-End Learning"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/#abstract","text":"End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning systems are specifically designed so that all modules are differentiable\uff08\u53ef\u5fae\u7684\uff09. In effect, not only a central learning machine , but also all \u201cperipheral\u201d modules like representation learning and memory formation are covered by a holistic\uff08\u6574\u4f53\u7684\uff09 learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning. Keywords: end-to-end machine learning","title":"Abstract"},{"location":"Theory/Deep-learning/End-to-end/End-to-end/#introduction","text":"We are today in the position to train rather deep and complex neural networks in an end-to-end (e2e) fashion, by gradient descent. In a nutshell, this amounts to\uff08\u76f8\u5f53\u4e8e\uff09 scaling up the good old backpropagation algorithm (see Schmidhuber, 2015 and references therein) to immensely rich and complex models. However, the end-to-end learning philosophy goes one step further: carefully ensuring that all modules of a learning systems are differentiable with respect to all adjustable parameters (weights) and training this system as a whole are lifted to the status of principles. This elegant although straightforward and somewhat brute-force technique has been popularized in the context of deep learning. It is a seemingly natural consequence of deep neural architectures blurring the classic boundaries between learning machine and other processing components by casting a possibly complex processing pipeline into the coherent and flexible modeling language of neural networks.1 The approach yields state-of-the-art results (Collobert et al., 2011; Krizhevsky et al., 2012; Mnih et al., 2015). Its appeal is a unified training scheme that makes most of the available information by taking labels (supervised learning) and rewards (reinforcement learning) into account, instead of relying only on the input distribution (unsupervised pre-training). Excellent recent examples of studies","title":"Introduction"},{"location":"Theory/Deep-learning/End-to-end/wikipedia-End-to-end-reinforcement-learning/","text":"End-to-end reinforcement learning End-to-end reinforcement learning #","title":"End-to-end-reinforcement-learning"},{"location":"Theory/Deep-learning/End-to-end/wikipedia-End-to-end-reinforcement-learning/#end-to-end-reinforcement-learning","text":"","title":"End-to-end reinforcement learning"},{"location":"Theory/Feature-engineering/Data-transformation(statistics)/","text":"Data transformation (statistics) Category:Statistical data transformation Data transformation (statistics) # Category:Statistical data transformation #","title":"Data-transformation(statistics)"},{"location":"Theory/Feature-engineering/Data-transformation(statistics)/#data-transformation-statistics","text":"","title":"Data transformation (statistics)"},{"location":"Theory/Feature-engineering/Data-transformation(statistics)/#categorystatistical-data-transformation","text":"","title":"Category:Statistical data transformation"},{"location":"Theory/Feature-engineering/Feature-scaling/","text":"Feature scaling Motivation Methods Rescaling (min-max normalization) Mean normalization Standardization (Z-score Normalization) Scaling to unit length Application Feature scaling # Feature scaling is a method used to normalize\uff08\u6b63\u89c4\u5316\uff0c\u6807\u51c6\u5316\uff09 the range of independent variables or features of data. In data processing , it is also known as data normalization and is generally performed during the data preprocessing step. Motivation # Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization . For example, many classifiers calculate the distance between two points by the Euclidean distance . If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.[ 1] Methods # Rescaling (min-max normalization) # Also known as min-max scaling or min-max normalization, is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [\u22121, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as: $ x'={\\frac {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ x $ is an original value, $ x' $ is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights). To rescale a range between an arbitrary set of values [a, b], the formula becomes: $ x'=a+{\\frac {(x-{\\text{min}}(x))(b-a)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ a,b $ are the min-max values. Mean normalization # $ x'={\\frac {x-{\\text{average}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ x $ is an original value, $ x' $ is the normalized value. There is another form of the mean normalization which is when we divide by the standard deviation which is also called standardization. Standardization (Z-score Normalization) # In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions . Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance . This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines , logistic regression , and artificial neural networks )[ 2] [ citation needed ]. The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation\uff08\u6807\u51c6\u5dee\uff09. $ x'={\\frac {x-{\\bar {x}}}{\\sigma }} $ Where $ x $ is the original feature vector, $ {\\bar {x}}={\\text{average}}(x) $ is the mean of that feature vector, and $ \\sigma $ is its standard deviation. Scaling to unit length # Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector: $ x'={\\frac {x}{\\left|{x}\\right|}} $ In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or Taxicab Geometry ) of the feature vector. This is especially important if in the following learning steps the Scalar Metric is used as a distance measure. Application # In stochastic gradient descent , feature scaling can sometimes improve the convergence speed of the algorithm[ 2] [ citation needed ]. In support vector machines,[ 3] it can reduce the time to find support vectors. Note that feature scaling changes the SVM result[ citation needed ].","title":"Feature-scaling"},{"location":"Theory/Feature-engineering/Feature-scaling/#feature-scaling","text":"Feature scaling is a method used to normalize\uff08\u6b63\u89c4\u5316\uff0c\u6807\u51c6\u5316\uff09 the range of independent variables or features of data. In data processing , it is also known as data normalization and is generally performed during the data preprocessing step.","title":"Feature scaling"},{"location":"Theory/Feature-engineering/Feature-scaling/#motivation","text":"Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization . For example, many classifiers calculate the distance between two points by the Euclidean distance . If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.[ 1]","title":"Motivation"},{"location":"Theory/Feature-engineering/Feature-scaling/#methods","text":"","title":"Methods"},{"location":"Theory/Feature-engineering/Feature-scaling/#rescaling-min-max-normalization","text":"Also known as min-max scaling or min-max normalization, is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [\u22121, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as: $ x'={\\frac {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ x $ is an original value, $ x' $ is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights). To rescale a range between an arbitrary set of values [a, b], the formula becomes: $ x'=a+{\\frac {(x-{\\text{min}}(x))(b-a)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ a,b $ are the min-max values.","title":"Rescaling (min-max normalization)"},{"location":"Theory/Feature-engineering/Feature-scaling/#mean-normalization","text":"$ x'={\\frac {x-{\\text{average}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} $ where $ x $ is an original value, $ x' $ is the normalized value. There is another form of the mean normalization which is when we divide by the standard deviation which is also called standardization.","title":"Mean normalization"},{"location":"Theory/Feature-engineering/Feature-scaling/#standardization-z-score-normalization","text":"In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions . Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance . This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines , logistic regression , and artificial neural networks )[ 2] [ citation needed ]. The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation\uff08\u6807\u51c6\u5dee\uff09. $ x'={\\frac {x-{\\bar {x}}}{\\sigma }} $ Where $ x $ is the original feature vector, $ {\\bar {x}}={\\text{average}}(x) $ is the mean of that feature vector, and $ \\sigma $ is its standard deviation.","title":"Standardization (Z-score Normalization)"},{"location":"Theory/Feature-engineering/Feature-scaling/#scaling-to-unit-length","text":"Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector: $ x'={\\frac {x}{\\left|{x}\\right|}} $ In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or Taxicab Geometry ) of the feature vector. This is especially important if in the following learning steps the Scalar Metric is used as a distance measure.","title":"Scaling to unit length"},{"location":"Theory/Feature-engineering/Feature-scaling/#application","text":"In stochastic gradient descent , feature scaling can sometimes improve the convergence speed of the algorithm[ 2] [ citation needed ]. In support vector machines,[ 3] it can reduce the time to find support vectors. Note that feature scaling changes the SVM result[ citation needed ].","title":"Application"},{"location":"Theory/Feature-engineering/Normalization(statistics)/","text":"Normalization (statistics) Normalization (statistics) #","title":"Normalization(statistics)"},{"location":"Theory/Feature-engineering/Normalization(statistics)/#normalization-statistics","text":"","title":"Normalization (statistics)"},{"location":"Theory/Markov-model/Forward-algorithm/","text":"Forward algorithm Forward algorithm #","title":"Forward-algorithm"},{"location":"Theory/Markov-model/Forward-algorithm/#forward-algorithm","text":"","title":"Forward algorithm"},{"location":"Theory/Markov-model/Hidden-Markov-model/","text":"Hidden Markov model #","title":"Hidden-Markov-model"},{"location":"Theory/Markov-model/Hidden-Markov-model/#hidden-markov-model","text":"","title":"Hidden Markov model"},{"location":"Theory/Markov-model/Markov-chain/","text":"Markov chain","title":"Markov-chain"},{"location":"Theory/Markov-model/Markov-models/","text":"Category:Markov models Category:Markov models #","title":"Markov-models"},{"location":"Theory/Markov-model/Markov-models/#categorymarkov-models","text":"","title":"Category:Markov models"},{"location":"Theory/Markov-model/Viterbi-algorithm/","text":"Viterbi algorithm application Viterbi algorithm # application # jieba","title":"Viterbi-algorithm"},{"location":"Theory/Markov-model/Viterbi-algorithm/#viterbi-algorithm","text":"","title":"Viterbi algorithm"},{"location":"Theory/Markov-model/Viterbi-algorithm/#application","text":"jieba","title":"application"},{"location":"Theory/Probability-theory-and-Statistics/","text":"\u5173\u4e8e\u672c\u7ae0 # \u672c\u7ae0\u4e3b\u8981\u63cf\u8ff0machine learning\u4e2d\u5173\u4e8eProbability theory\uff08\u6982\u7387\u8bba\uff09\u3001Statistics\uff08\u7edf\u8ba1\u5b66\uff09\u7684\u4e00\u4e9b\u57fa\u7840\u77e5\u8bc6\u3002","title":"Introduction"},{"location":"Theory/Probability-theory-and-Statistics/#_1","text":"\u672c\u7ae0\u4e3b\u8981\u63cf\u8ff0machine learning\u4e2d\u5173\u4e8eProbability theory\uff08\u6982\u7387\u8bba\uff09\u3001Statistics\uff08\u7edf\u8ba1\u5b66\uff09\u7684\u4e00\u4e9b\u57fa\u7840\u77e5\u8bc6\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/","text":"Probability theory # \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Probability theory \u7ef4\u57fa\u767e\u79d1 Probability theory \uff1a Probability theory is the branch of mathematics concerned with probability . Although there are several different probability interpretations , probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms (\u516c\u7406). Typically these axioms formalise probability in terms of a probability space , which assigns a measure taking values between 0 and 1, termed the probability measure , to a set of outcomes called the sample space . Any specified subset of these outcomes is called an event . Central subjects in probability theory include discrete and continuous random variables , probability distributions , and stochastic processes , which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion. Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem . As a mathematical foundation for statistics , probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics . A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantuim mechanics . Random variable # \u968f\u673a\u53d8\u91cf \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Random variable Deep Learning Book \u7684chapter 3.2 Random Variables Deep Learning Book \u7684chapter 3.2 Random Variables\uff1a A random variable is a variable that can take on different values randomly. Random variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states. Note that these states are not necessarily the integers; they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value. Probability Distributions # \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 probability distributions Deep Learning Book \u76843.3 Probability Distributions Deep Learning Book \u76843.3 Probability Distributions: A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous. A probability distribution over discrete variables may be described using a probability mass function (PMF). When working with continuous random variables, we describe probability distributions using a probability density function (PDF) rather than a probability mass function. Common probability distributions and their applications # Stochastic process # \u53c2\u89c1\u300a Stochastic-process \u300b","title":"Probability-theory"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/#probability-theory","text":"\u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Probability theory \u7ef4\u57fa\u767e\u79d1 Probability theory \uff1a Probability theory is the branch of mathematics concerned with probability . Although there are several different probability interpretations , probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms (\u516c\u7406). Typically these axioms formalise probability in terms of a probability space , which assigns a measure taking values between 0 and 1, termed the probability measure , to a set of outcomes called the sample space . Any specified subset of these outcomes is called an event . Central subjects in probability theory include discrete and continuous random variables , probability distributions , and stochastic processes , which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion. Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem . As a mathematical foundation for statistics , probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics . A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantuim mechanics .","title":"Probability theory"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/#random-variable","text":"\u968f\u673a\u53d8\u91cf \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Random variable Deep Learning Book \u7684chapter 3.2 Random Variables Deep Learning Book \u7684chapter 3.2 Random Variables\uff1a A random variable is a variable that can take on different values randomly. Random variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states. Note that these states are not necessarily the integers; they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value.","title":"Random variable"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/#probability-distributions","text":"\u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 probability distributions Deep Learning Book \u76843.3 Probability Distributions Deep Learning Book \u76843.3 Probability Distributions: A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous. A probability distribution over discrete variables may be described using a probability mass function (PMF). When working with continuous random variables, we describe probability distributions using a probability density function (PDF) rather than a probability mass function.","title":"Probability Distributions"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/#common-probability-distributions-and-their-applications","text":"","title":"Common probability distributions and their applications"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Probability-theory/#stochastic-process","text":"\u53c2\u89c1\u300a Stochastic-process \u300b","title":"Stochastic process"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Independent-and-identically-distributed-random-variables/","text":"Independent and identically distributed random variables #","title":"Independent-and-identically-distributed-random-variables"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Independent-and-identically-distributed-random-variables/#independent-and-identically-distributed-random-variables","text":"","title":"Independent and identically distributed random variables"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Markov-chain/","text":"Markov chain # \u4e3b\u8981\u53c2\u8003\uff1a \u7ef4\u57fa\u767e\u79d1 Markov chain \u7ef4\u57fa\u767e\u79d1 Markov chain : A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.[ 1] [ 2] [ 3] In continuous-time , it is known as a Markov process.","title":"Markov-chain"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Markov-chain/#markov-chain","text":"\u4e3b\u8981\u53c2\u8003\uff1a \u7ef4\u57fa\u767e\u79d1 Markov chain \u7ef4\u57fa\u767e\u79d1 Markov chain : A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.[ 1] [ 2] [ 3] In continuous-time , it is known as a Markov process.","title":"Markov chain"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Stochastic-process/","text":"Stochastic process # \u968f\u673a\u8fc7\u7a0b \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Stochastic process \u5bf9stochastic process\u7684\u76f4\u89c2\u611f\u53d7\u662f\u5b83\u662f\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u8fd9\u4e2a\u6570\u5b66\u6a21\u578b\u4e2d\u6d89\u53ca\u591a\u4e2arandom variable\u3002\u663e\u793a\u4e16\u754c\u4e2d\u7684\u5f88\u591a\u4e8b\u7269\u90fd\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5355\u4e2arandom variable\u662f\u975e\u5e38\u96be\u4ee5\u63cf\u8ff0\u7684\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528stochastic process\u3002 \u7ef4\u57fa\u767e\u79d1 Stochastic process \uff1a In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables . Historically, the random variables were associated with or indexed by a set of numbers, usually viewed as points in time, giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time , such as the growth of a bacterial population, an electrical current fluctuating due to thermal noise , or the movement of a gas molecule . Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples # Bernoulli process","title":"Stochastic-process"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Stochastic-process/#stochastic-process","text":"\u968f\u673a\u8fc7\u7a0b \u53c2\u8003\u5185\u5bb9\uff1a \u7ef4\u57fa\u767e\u79d1 Stochastic process \u5bf9stochastic process\u7684\u76f4\u89c2\u611f\u53d7\u662f\u5b83\u662f\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u8fd9\u4e2a\u6570\u5b66\u6a21\u578b\u4e2d\u6d89\u53ca\u591a\u4e2arandom variable\u3002\u663e\u793a\u4e16\u754c\u4e2d\u7684\u5f88\u591a\u4e8b\u7269\u90fd\u662f\u975e\u5e38\u590d\u6742\u7684\uff0c\u5355\u4e2arandom variable\u662f\u975e\u5e38\u96be\u4ee5\u63cf\u8ff0\u7684\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528stochastic process\u3002 \u7ef4\u57fa\u767e\u79d1 Stochastic process \uff1a In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables . Historically, the random variables were associated with or indexed by a set of numbers, usually viewed as points in time, giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time , such as the growth of a bacterial population, an electrical current fluctuating due to thermal noise , or the movement of a gas molecule . Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner.","title":"Stochastic process"},{"location":"Theory/Probability-theory-and-Statistics/Probability-theory/Stochastic-process/Stochastic-process/#examples","text":"Bernoulli process","title":"Examples"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/","text":"\u5173\u4e8e\u672c\u7ae0 # \u5728\u9605\u8bfbmachine learning\u76f8\u5173\u7684\u5185\u5bb9\u7684\u65f6\u5019\uff0c\u9891\u9891\u9047\u5230\u7edf\u8ba1\u5b66\uff08 Statistics \uff09\u76f8\u5173\u7684\u7684\u672f\u8bed\uff0c\u6545\u51b3\u5b9a\u5bf9\u7edf\u8ba1\u5b66\u7684\u5185\u5bb9\u8fdb\u884c\u4e00\u4e0b\u68b3\u7406\u3002","title":"Introduction"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/#_1","text":"\u5728\u9605\u8bfbmachine learning\u76f8\u5173\u7684\u5185\u5bb9\u7684\u65f6\u5019\uff0c\u9891\u9891\u9047\u5230\u7edf\u8ba1\u5b66\uff08 Statistics \uff09\u76f8\u5173\u7684\u7684\u672f\u8bed\uff0c\u6545\u51b3\u5b9a\u5bf9\u7edf\u8ba1\u5b66\u7684\u5185\u5bb9\u8fdb\u884c\u4e00\u4e0b\u68b3\u7406\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/","text":"Statistical inference # \u7edf\u8ba1\u63a8\u65ad\u3002 \u672c\u6587\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Statistical inference \u7684\u7b14\u8bb0\u3002 Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution . \u8fd9\u6bb5\u8bdd\u56de\u7b54\u4e86statistical inference\u6240\u201cinfer\u201d\u7684\uff1aproperties of an underlying probability distribution \u3002\u6211\u4eec\u5047\u8bbe\u7814\u7a76\u7684\u95ee\u9898\u662f\u5b58\u5728\u7740\u4e00\u4e2adistribution\u7684\uff0c\u6211\u4eec\u901a\u8fc7 Statistical inference \u53ef\u4ee5\u5b66\u4e60\u5230\u8fd9\u4e2adistribution\u3002 It is assumed that the observed data set is sampled from a larger population. \u8fd9\u662fstatistical inference\u7684\u524d\u63d0\u3002 machine learning\u672c\u8d28\u4e0a\u662f\u4e00\u79cdstatistical inference\u3002 Introduction # Statistical inference makes propositions about a population, using data drawn from the population with some form of sampling . Given a hypothesis about a population, for which we wish to draw inferences, statistical inference consists of (first) selecting a statistical model of the process that generates the data (second) deducing propositions from the model. \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling\" \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\". Paradigms for inference # Frequentist inference # Bayesian Inference # Likelihood-based inference # Main article: Likelihoodism","title":"Statistical-inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#statistical-inference","text":"\u7edf\u8ba1\u63a8\u65ad\u3002 \u672c\u6587\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Statistical inference \u7684\u7b14\u8bb0\u3002 Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution . \u8fd9\u6bb5\u8bdd\u56de\u7b54\u4e86statistical inference\u6240\u201cinfer\u201d\u7684\uff1aproperties of an underlying probability distribution \u3002\u6211\u4eec\u5047\u8bbe\u7814\u7a76\u7684\u95ee\u9898\u662f\u5b58\u5728\u7740\u4e00\u4e2adistribution\u7684\uff0c\u6211\u4eec\u901a\u8fc7 Statistical inference \u53ef\u4ee5\u5b66\u4e60\u5230\u8fd9\u4e2adistribution\u3002 It is assumed that the observed data set is sampled from a larger population. \u8fd9\u662fstatistical inference\u7684\u524d\u63d0\u3002 machine learning\u672c\u8d28\u4e0a\u662f\u4e00\u79cdstatistical inference\u3002","title":"Statistical inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#introduction","text":"Statistical inference makes propositions about a population, using data drawn from the population with some form of sampling . Given a hypothesis about a population, for which we wish to draw inferences, statistical inference consists of (first) selecting a statistical model of the process that generates the data (second) deducing propositions from the model. \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling\" \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".","title":"Introduction"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#paradigms-for-inference","text":"","title":"Paradigms for inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#frequentist-inference","text":"","title":"Frequentist inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#bayesian-inference","text":"","title":"Bayesian Inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-inference/#likelihood-based-inference","text":"Main article: Likelihoodism","title":"Likelihood-based inference"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-model/","text":"Statistical model # \u672c\u6587\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Statistical model \u7684\u7b14\u8bb0\u3002 A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population ). A statistical model represents, often in considerably idealized form, the data-generating process. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6709\u5982\u4e0b\u5185\u5bb9\u6ca1\u6709\u7406\u89e3\uff1a \u201cdata-generating process\u201d A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. \u8fd9\u662f\u539f\u6587\u4e2d\uff0c\u6700\u6e05\u695a\u3001\u7b80\u6d01\u7684\u89e3\u91ca\uff0c\u7b80\u5355\u7406\u89e3\u662f\uff1astatistical model\u5176\u5b9e\u4e00\u4e2a\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u662f\u5173\u4e8erandom variable\u7684\u3002Statistical model\u7684\u6700\u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f linear regression \u3002 Formal definition #","title":"Statistical-model"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-model/#statistical-model","text":"\u672c\u6587\u662f\u9605\u8bfb\u7ef4\u57fa\u767e\u79d1 Statistical model \u7684\u7b14\u8bb0\u3002 A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population ). A statistical model represents, often in considerably idealized form, the data-generating process. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6709\u5982\u4e0b\u5185\u5bb9\u6ca1\u6709\u7406\u89e3\uff1a \u201cdata-generating process\u201d A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. \u8fd9\u662f\u539f\u6587\u4e2d\uff0c\u6700\u6e05\u695a\u3001\u7b80\u6d01\u7684\u89e3\u91ca\uff0c\u7b80\u5355\u7406\u89e3\u662f\uff1astatistical model\u5176\u5b9e\u4e00\u4e2a\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u662f\u5173\u4e8erandom variable\u7684\u3002Statistical model\u7684\u6700\u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f linear regression \u3002","title":"Statistical model"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistical-model/#formal-definition","text":"","title":"Formal definition"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/","text":"Statistics\uff08\u7edf\u8ba1\u5b66\uff09 # \u672c\u6587\u57fa\u4e8e\uff1a \u7ef4\u57fa\u767e\u79d1 Statistics Mathematical statistics # \u6570\u7406\u7edf\u8ba1\u3002 Mathematical statistics is the application of mathematics to statistics. Statistical population and Sample (statistics) # \u201c\u603b\u4f53\u201d\u4e0e\u201c\u6837\u672c\u201d\uff0c\u8fd9\u662f\u7edf\u8ba1\u5b66\u4e2d\u7684\u975e\u5e38\u91cd\u8981\u7684\u4e24\u4e2a\u6982\u5ff5\u3002 \u7edf\u8ba1\u5b66\u7814\u7a76\u4ec0\u4e48\uff1f # \u611f\u89c9\u6574\u4e2a\u7edf\u8ba1\u5b66\u6240\u505a\u7684\u4e8b\u60c5\u662f\uff1a\u5bf9sample\u8fdb\u884c\u5b66\u4e60\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u7ed3\u679c\u5e94\u7528\u4e8e\uff08\u63a8\u5e7f\u5230\uff09 population\u3002 Two main statistical methods # \u201cstatistical method\u201d\u7684\u542b\u4e49\u662f\u5982\u4f55\u8fdb\u884c\u7edf\u8ba1\u3002 Descriptive statistics # \u63cf\u8ff0\u6027\u7edf\u8ba1\uff0c\u987e\u540d\u601d\u4e49\uff0c\u5b83\u4f7f\u7528\u4ecesample\u4e2d\u5b66\u4e60\u5230\u7684\u4e00\u4e9b indexes \uff08\u6307\u6807\uff09\uff0c\u6bd4\u5982 mean \u6216 standard deviation \u6765 \u63cf\u8ff0 population\u3002\u8fd9\u4e9b\u6307\u6807\u662f\u201cdistribution\u201d \u7684 property ,\u6b63\u5982 Statistics \u4e2d\u6240\u8a00\uff1a Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location ) seeks to characterize the distribution's central or typical value, while dispersion (or variability ) characterizes the extent to which members of the distribution depart from its center and each other. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u201cproperties of distribution\u201d\u662f\u6307 Probability distribution \u3002 Inferential statistics # \u53c2\u89c1\u300a Statistical-inference \u300b Statistical hypothesis testing # \u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u76f8\u5173\u6027\u6d4b\u8bd5\uff1a\u5047\u8bbe\u4e24\u8005\u76f8\u5173\uff0c\u7136\u540e\u8fdb\u884c\u6d4b\u8bd5\u3002","title":"Statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#statistics","text":"\u672c\u6587\u57fa\u4e8e\uff1a \u7ef4\u57fa\u767e\u79d1 Statistics","title":"Statistics\uff08\u7edf\u8ba1\u5b66\uff09"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#mathematical-statistics","text":"\u6570\u7406\u7edf\u8ba1\u3002 Mathematical statistics is the application of mathematics to statistics.","title":"Mathematical statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#statistical-population-and-sample-statistics","text":"\u201c\u603b\u4f53\u201d\u4e0e\u201c\u6837\u672c\u201d\uff0c\u8fd9\u662f\u7edf\u8ba1\u5b66\u4e2d\u7684\u975e\u5e38\u91cd\u8981\u7684\u4e24\u4e2a\u6982\u5ff5\u3002","title":"Statistical population and Sample (statistics)"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#_1","text":"\u611f\u89c9\u6574\u4e2a\u7edf\u8ba1\u5b66\u6240\u505a\u7684\u4e8b\u60c5\u662f\uff1a\u5bf9sample\u8fdb\u884c\u5b66\u4e60\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u7ed3\u679c\u5e94\u7528\u4e8e\uff08\u63a8\u5e7f\u5230\uff09 population\u3002","title":"\u7edf\u8ba1\u5b66\u7814\u7a76\u4ec0\u4e48\uff1f"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#two-main-statistical-methods","text":"\u201cstatistical method\u201d\u7684\u542b\u4e49\u662f\u5982\u4f55\u8fdb\u884c\u7edf\u8ba1\u3002","title":"Two main statistical methods"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#descriptive-statistics","text":"\u63cf\u8ff0\u6027\u7edf\u8ba1\uff0c\u987e\u540d\u601d\u4e49\uff0c\u5b83\u4f7f\u7528\u4ecesample\u4e2d\u5b66\u4e60\u5230\u7684\u4e00\u4e9b indexes \uff08\u6307\u6807\uff09\uff0c\u6bd4\u5982 mean \u6216 standard deviation \u6765 \u63cf\u8ff0 population\u3002\u8fd9\u4e9b\u6307\u6807\u662f\u201cdistribution\u201d \u7684 property ,\u6b63\u5982 Statistics \u4e2d\u6240\u8a00\uff1a Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location ) seeks to characterize the distribution's central or typical value, while dispersion (or variability ) characterizes the extent to which members of the distribution depart from its center and each other. \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\u201cproperties of distribution\u201d\u662f\u6307 Probability distribution \u3002","title":"Descriptive statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#inferential-statistics","text":"\u53c2\u89c1\u300a Statistical-inference \u300b","title":"Inferential statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/Statistics/#statistical-hypothesis-testing","text":"\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u76f8\u5173\u6027\u6d4b\u8bd5\uff1a\u5047\u8bbe\u4e24\u8005\u76f8\u5173\uff0c\u7136\u540e\u8fdb\u884c\u6d4b\u8bd5\u3002","title":"Statistical hypothesis testing"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/VS-statistical-interference-VS-descriptive-statistics/","text":"VS statistical interference VS descriptive statistics # \u5728 Statistical inference \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Inferential statistics can be contrasted with descriptive statistics . Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.","title":"VS-statistical-interference-VS-descriptive-statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/VS-statistical-interference-VS-descriptive-statistics/#vs-statistical-interference-vs-descriptive-statistics","text":"\u5728 Statistical inference \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Inferential statistics can be contrasted with descriptive statistics . Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.","title":"VS statistical interference VS descriptive statistics"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/wikipedia-Conditional-independence/","text":"Conditional independence #","title":"[Conditional independence](https://en.wikipedia.org/wiki/Conditional_independence)"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/wikipedia-Conditional-independence/#conditional-independence","text":"","title":"Conditional independence"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/wikipedia-Graphical-model/","text":"Graphical model #","title":"[Graphical model](https://en.wikipedia.org/wiki/Graphical_model)"},{"location":"Theory/Probability-theory-and-Statistics/Statistics/wikipedia-Graphical-model/#graphical-model","text":"","title":"Graphical model"}]}